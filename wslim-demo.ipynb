{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/.local/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (2.2.3) or chardet (3.0.4) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'OWDETR': ('aeroplane', 'bicycle', 'bird', 'boat', 'bus', 'car', 'cat', 'cow', 'dog', 'horse', 'motorbike', 'sheep', 'train', 'elephant', 'bear', 'zebra', 'giraffe', 'truck', 'person', 'traffic light', 'fire hydrant', 'stop sign', 'parking meter', 'bench', 'chair', 'diningtable', 'pottedplant', 'backpack', 'umbrella', 'handbag', 'tie', 'suitcase', 'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'bed', 'toilet', 'sofa', 'frisbee', 'skis', 'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket', 'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone', 'book', 'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'tvmonitor', 'bottle', 'unknown'), 'TOWOD': ('aeroplane', 'bicycle', 'bird', 'boat', 'bottle', 'bus', 'car', 'cat', 'chair', 'cow', 'diningtable', 'dog', 'horse', 'motorbike', 'person', 'pottedplant', 'sheep', 'sofa', 'train', 'tvmonitor', 'truck', 'traffic light', 'fire hydrant', 'stop sign', 'parking meter', 'bench', 'elephant', 'bear', 'zebra', 'giraffe', 'backpack', 'umbrella', 'handbag', 'tie', 'suitcase', 'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'frisbee', 'skis', 'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket', 'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'bed', 'toilet', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone', 'book', 'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'unknown'), 'VOC2007': ('aeroplane', 'bicycle', 'bird', 'boat', 'bottle', 'bus', 'car', 'cat', 'chair', 'cow', 'diningtable', 'dog', 'horse', 'motorbike', 'person', 'pottedplant', 'sheep', 'sofa', 'train', 'tvmonitor', 'truck', 'traffic light', 'fire hydrant', 'stop sign', 'parking meter', 'bench', 'elephant', 'bear', 'zebra', 'giraffe', 'backpack', 'umbrella', 'handbag', 'tie', 'suitcase', 'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'frisbee', 'skis', 'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket', 'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'bed', 'toilet', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone', 'book', 'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'unknown')}\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import datetime\n",
    "import json\n",
    "import random\n",
    "import time\n",
    "from pathlib import Path\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import datasets\n",
    "import util.misc as utils\n",
    "import datasets.samplers as samplers\n",
    "from datasets import build_dataset, get_coco_api_from_dataset\n",
    "from datasets.coco import make_coco_transforms\n",
    "from datasets.torchvision_datasets.open_world import OWDetection\n",
    "# from engine import evaluate, train_one_epoch, get_exemplar_replay\n",
    "from models import build_model\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Arguments setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotmap import DotMap\n",
    "\n",
    "args = DotMap(\n",
    "    # main code\n",
    "    frozen_weights=None,\n",
    "    seed=0,\n",
    "    device='cuda',\n",
    "    data_root='./data/OWOD',\n",
    "    batch_size=2,\n",
    "    num_workers=8,\n",
    "    lr_linear_proj_names=['reference_points', 'sampling_offsets'],\n",
    "    lr=2e-4,\n",
    "    lr_backbone_names=[\"backbone.0\"],\n",
    "    lr_linear_proj_mult=2e-5,\n",
    "    sgd=False,\n",
    "    weight_decay=1e-4,    \n",
    "    pretrain=None,\n",
    "    freeze_prob_model=False,\n",
    "    \n",
    "    # build_model\n",
    "    num_classes=81,\n",
    "    num_queries=100,\n",
    "    num_feature_levels=4,\n",
    "    aux_loss=True,\n",
    "    with_box_refine=False,\n",
    "    two_stage=False,\n",
    "    masks=False,\n",
    "    cls_loss_coef=2,\n",
    "    bbox_loss_coef=5,\n",
    "    giou_loss_coef=2,\n",
    "    # obj_loss_coef=1, # duplicated\n",
    "    # mask_loss_coef=,\n",
    "    # dice_loss_coef=,\n",
    "    dec_layers=6,\n",
    "    hidden_dim=256,\n",
    "    focal_alpha=0.25,\n",
    "    # obj_temp=1, # duplicated\n",
    "    # dataset_file=,\n",
    "    \n",
    "    # build_backbone\n",
    "    lr_backbone=2e-5,\n",
    "    backbone='dino_resnet50',\n",
    "    dilation=False,\n",
    "    position_embedding='sine',\n",
    "    \n",
    "    # build_deforamble_transformer\n",
    "    nheads=8,\n",
    "    enc_layers=6,\n",
    "    dim_feedforward=1024,\n",
    "    dropout=0.1,\n",
    "    dec_n_points=4,\n",
    "    enc_n_points=4,\n",
    "    \n",
    "    # build_matcher\n",
    "    set_cost_class=2,\n",
    "    set_cost_bbox=5,\n",
    "    set_cost_giou=2,\n",
    "    \n",
    "    # bash script\n",
    "    output_dir='output',\n",
    "    dataset='TOWOD',\n",
    "    PREV_INTRODUCED_CLS=0,\n",
    "    CUR_INTRODUCED_CLS=5,\n",
    "    train_set='owod_t1_toy_5classes_train', \n",
    "    test_set='owod_all_task_test',\n",
    "    epochs=5,\n",
    "    model_type='prob',\n",
    "    obj_loss_coef=8e-4,\n",
    "    obj_temp=1.3,\n",
    "    exemplar_replay_selection=True,\n",
    "    exemplar_replay_max_length=850,\n",
    "    exemplar_replay_dir='',\n",
    "    exemplar_replay_cur_file='',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Before Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions from main_open_world.py\n",
    "def get_datasets(args):\n",
    "    print(args.dataset)\n",
    "\n",
    "    train_set = args.train_set\n",
    "    test_set = args.test_set\n",
    "    dataset_train = OWDetection(args, args.data_root, image_set=args.train_set, transforms=make_coco_transforms(args.train_set), dataset = args.dataset)\n",
    "    dataset_val = OWDetection(args, args.data_root, image_set=args.test_set, dataset = args.dataset, transforms=make_coco_transforms(args.test_set))\n",
    "\n",
    "    print(args.train_set)\n",
    "    print(args.test_set)\n",
    "    print(dataset_train)\n",
    "    print(dataset_val)\n",
    "\n",
    "    return dataset_train, dataset_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DotMap(frozen_weights=None, seed=0, device='cuda', data_root='./data/OWOD', batch_size=2, num_workers=8, lr_linear_proj_names=['reference_points', 'sampling_offsets'], lr=0.0002, lr_backbone_names=['backbone.0'], lr_linear_proj_mult=2e-05, sgd=False, weight_decay=0.0001, pretrain=None, freeze_prob_model=False, num_classes=81, num_queries=100, num_feature_levels=4, aux_loss=True, with_box_refine=False, two_stage=False, masks=False, cls_loss_coef=2, bbox_loss_coef=5, giou_loss_coef=2, dec_layers=6, hidden_dim=256, focal_alpha=0.25, lr_backbone=2e-05, backbone='dino_resnet50', dilation=False, position_embedding='sine', nheads=8, enc_layers=6, dim_feedforward=1024, dropout=0.1, dec_n_points=4, enc_n_points=4, set_cost_class=2, set_cost_bbox=5, set_cost_giou=2, output_dir='output', dataset='TOWOD', PREV_INTRODUCED_CLS=0, CUR_INTRODUCED_CLS=5, train_set='owod_t1_toy_5classes_train', test_set='owod_all_task_test', epochs=5, model_type='prob', obj_loss_coef=0.0008, obj_temp=1.3, exemplar_replay_selection=True, exemplar_replay_max_length=850, exemplar_replay_dir='', exemplar_replay_cur_file='')\n",
      "Invalid class range: [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79]\n",
      "DINO resnet50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/.local/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/workspace/.local/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running with exemplar_replay_selection\n",
      "DeformableDETR(\n",
      "  (transformer): DeformableTransformer(\n",
      "    (encoder): DeformableTransformerEncoder(\n",
      "      (layers): ModuleList(\n",
      "        (0): DeformableTransformerEncoderLayer(\n",
      "          (self_attn): MSDeformAttn(\n",
      "            (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (attention_weights): Linear(in_features=256, out_features=128, bias=True)\n",
      "            (value_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (output_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "          )\n",
      "          (dropout1): Dropout(p=0.1, inplace=False)\n",
      "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (linear1): Linear(in_features=256, out_features=1024, bias=True)\n",
      "          (dropout2): Dropout(p=0.1, inplace=False)\n",
      "          (linear2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "          (dropout3): Dropout(p=0.1, inplace=False)\n",
      "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (1): DeformableTransformerEncoderLayer(\n",
      "          (self_attn): MSDeformAttn(\n",
      "            (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (attention_weights): Linear(in_features=256, out_features=128, bias=True)\n",
      "            (value_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (output_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "          )\n",
      "          (dropout1): Dropout(p=0.1, inplace=False)\n",
      "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (linear1): Linear(in_features=256, out_features=1024, bias=True)\n",
      "          (dropout2): Dropout(p=0.1, inplace=False)\n",
      "          (linear2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "          (dropout3): Dropout(p=0.1, inplace=False)\n",
      "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (2): DeformableTransformerEncoderLayer(\n",
      "          (self_attn): MSDeformAttn(\n",
      "            (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (attention_weights): Linear(in_features=256, out_features=128, bias=True)\n",
      "            (value_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (output_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "          )\n",
      "          (dropout1): Dropout(p=0.1, inplace=False)\n",
      "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (linear1): Linear(in_features=256, out_features=1024, bias=True)\n",
      "          (dropout2): Dropout(p=0.1, inplace=False)\n",
      "          (linear2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "          (dropout3): Dropout(p=0.1, inplace=False)\n",
      "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (3): DeformableTransformerEncoderLayer(\n",
      "          (self_attn): MSDeformAttn(\n",
      "            (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (attention_weights): Linear(in_features=256, out_features=128, bias=True)\n",
      "            (value_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (output_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "          )\n",
      "          (dropout1): Dropout(p=0.1, inplace=False)\n",
      "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (linear1): Linear(in_features=256, out_features=1024, bias=True)\n",
      "          (dropout2): Dropout(p=0.1, inplace=False)\n",
      "          (linear2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "          (dropout3): Dropout(p=0.1, inplace=False)\n",
      "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (4): DeformableTransformerEncoderLayer(\n",
      "          (self_attn): MSDeformAttn(\n",
      "            (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (attention_weights): Linear(in_features=256, out_features=128, bias=True)\n",
      "            (value_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (output_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "          )\n",
      "          (dropout1): Dropout(p=0.1, inplace=False)\n",
      "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (linear1): Linear(in_features=256, out_features=1024, bias=True)\n",
      "          (dropout2): Dropout(p=0.1, inplace=False)\n",
      "          (linear2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "          (dropout3): Dropout(p=0.1, inplace=False)\n",
      "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (5): DeformableTransformerEncoderLayer(\n",
      "          (self_attn): MSDeformAttn(\n",
      "            (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (attention_weights): Linear(in_features=256, out_features=128, bias=True)\n",
      "            (value_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (output_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "          )\n",
      "          (dropout1): Dropout(p=0.1, inplace=False)\n",
      "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (linear1): Linear(in_features=256, out_features=1024, bias=True)\n",
      "          (dropout2): Dropout(p=0.1, inplace=False)\n",
      "          (linear2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "          (dropout3): Dropout(p=0.1, inplace=False)\n",
      "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (decoder): DeformableTransformerDecoder(\n",
      "      (layers): ModuleList(\n",
      "        (0): DeformableTransformerDecoderLayer(\n",
      "          (cross_attn): MSDeformAttn(\n",
      "            (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (attention_weights): Linear(in_features=256, out_features=128, bias=True)\n",
      "            (value_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (output_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "          )\n",
      "          (dropout1): Dropout(p=0.1, inplace=False)\n",
      "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
      "          )\n",
      "          (dropout2): Dropout(p=0.1, inplace=False)\n",
      "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (linear1): Linear(in_features=256, out_features=1024, bias=True)\n",
      "          (dropout3): Dropout(p=0.1, inplace=False)\n",
      "          (linear2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "          (dropout4): Dropout(p=0.1, inplace=False)\n",
      "          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (1): DeformableTransformerDecoderLayer(\n",
      "          (cross_attn): MSDeformAttn(\n",
      "            (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (attention_weights): Linear(in_features=256, out_features=128, bias=True)\n",
      "            (value_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (output_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "          )\n",
      "          (dropout1): Dropout(p=0.1, inplace=False)\n",
      "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
      "          )\n",
      "          (dropout2): Dropout(p=0.1, inplace=False)\n",
      "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (linear1): Linear(in_features=256, out_features=1024, bias=True)\n",
      "          (dropout3): Dropout(p=0.1, inplace=False)\n",
      "          (linear2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "          (dropout4): Dropout(p=0.1, inplace=False)\n",
      "          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (2): DeformableTransformerDecoderLayer(\n",
      "          (cross_attn): MSDeformAttn(\n",
      "            (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (attention_weights): Linear(in_features=256, out_features=128, bias=True)\n",
      "            (value_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (output_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "          )\n",
      "          (dropout1): Dropout(p=0.1, inplace=False)\n",
      "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
      "          )\n",
      "          (dropout2): Dropout(p=0.1, inplace=False)\n",
      "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (linear1): Linear(in_features=256, out_features=1024, bias=True)\n",
      "          (dropout3): Dropout(p=0.1, inplace=False)\n",
      "          (linear2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "          (dropout4): Dropout(p=0.1, inplace=False)\n",
      "          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (3): DeformableTransformerDecoderLayer(\n",
      "          (cross_attn): MSDeformAttn(\n",
      "            (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (attention_weights): Linear(in_features=256, out_features=128, bias=True)\n",
      "            (value_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (output_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "          )\n",
      "          (dropout1): Dropout(p=0.1, inplace=False)\n",
      "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
      "          )\n",
      "          (dropout2): Dropout(p=0.1, inplace=False)\n",
      "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (linear1): Linear(in_features=256, out_features=1024, bias=True)\n",
      "          (dropout3): Dropout(p=0.1, inplace=False)\n",
      "          (linear2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "          (dropout4): Dropout(p=0.1, inplace=False)\n",
      "          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (4): DeformableTransformerDecoderLayer(\n",
      "          (cross_attn): MSDeformAttn(\n",
      "            (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (attention_weights): Linear(in_features=256, out_features=128, bias=True)\n",
      "            (value_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (output_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "          )\n",
      "          (dropout1): Dropout(p=0.1, inplace=False)\n",
      "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
      "          )\n",
      "          (dropout2): Dropout(p=0.1, inplace=False)\n",
      "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (linear1): Linear(in_features=256, out_features=1024, bias=True)\n",
      "          (dropout3): Dropout(p=0.1, inplace=False)\n",
      "          (linear2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "          (dropout4): Dropout(p=0.1, inplace=False)\n",
      "          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (5): DeformableTransformerDecoderLayer(\n",
      "          (cross_attn): MSDeformAttn(\n",
      "            (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (attention_weights): Linear(in_features=256, out_features=128, bias=True)\n",
      "            (value_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (output_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "          )\n",
      "          (dropout1): Dropout(p=0.1, inplace=False)\n",
      "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
      "          )\n",
      "          (dropout2): Dropout(p=0.1, inplace=False)\n",
      "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (linear1): Linear(in_features=256, out_features=1024, bias=True)\n",
      "          (dropout3): Dropout(p=0.1, inplace=False)\n",
      "          (linear2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "          (dropout4): Dropout(p=0.1, inplace=False)\n",
      "          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (reference_points): Linear(in_features=256, out_features=2, bias=True)\n",
      "  )\n",
      "  (class_embed): ModuleList(\n",
      "    (0): Linear(in_features=256, out_features=81, bias=True)\n",
      "    (1): Linear(in_features=256, out_features=81, bias=True)\n",
      "    (2): Linear(in_features=256, out_features=81, bias=True)\n",
      "    (3): Linear(in_features=256, out_features=81, bias=True)\n",
      "    (4): Linear(in_features=256, out_features=81, bias=True)\n",
      "    (5): Linear(in_features=256, out_features=81, bias=True)\n",
      "  )\n",
      "  (bbox_embed): ModuleList(\n",
      "    (0): MLP(\n",
      "      (layers): ModuleList(\n",
      "        (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "        (1): Linear(in_features=256, out_features=256, bias=True)\n",
      "        (2): Linear(in_features=256, out_features=4, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (1): MLP(\n",
      "      (layers): ModuleList(\n",
      "        (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "        (1): Linear(in_features=256, out_features=256, bias=True)\n",
      "        (2): Linear(in_features=256, out_features=4, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (2): MLP(\n",
      "      (layers): ModuleList(\n",
      "        (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "        (1): Linear(in_features=256, out_features=256, bias=True)\n",
      "        (2): Linear(in_features=256, out_features=4, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (3): MLP(\n",
      "      (layers): ModuleList(\n",
      "        (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "        (1): Linear(in_features=256, out_features=256, bias=True)\n",
      "        (2): Linear(in_features=256, out_features=4, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (4): MLP(\n",
      "      (layers): ModuleList(\n",
      "        (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "        (1): Linear(in_features=256, out_features=256, bias=True)\n",
      "        (2): Linear(in_features=256, out_features=4, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (5): MLP(\n",
      "      (layers): ModuleList(\n",
      "        (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "        (1): Linear(in_features=256, out_features=256, bias=True)\n",
      "        (2): Linear(in_features=256, out_features=4, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (prob_obj_head): ModuleList(\n",
      "    (0): ProbObjectnessHead(\n",
      "      (flatten): Flatten(start_dim=0, end_dim=1)\n",
      "      (objectness_bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "    )\n",
      "    (1): ProbObjectnessHead(\n",
      "      (flatten): Flatten(start_dim=0, end_dim=1)\n",
      "      (objectness_bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "    )\n",
      "    (2): ProbObjectnessHead(\n",
      "      (flatten): Flatten(start_dim=0, end_dim=1)\n",
      "      (objectness_bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "    )\n",
      "    (3): ProbObjectnessHead(\n",
      "      (flatten): Flatten(start_dim=0, end_dim=1)\n",
      "      (objectness_bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "    )\n",
      "    (4): ProbObjectnessHead(\n",
      "      (flatten): Flatten(start_dim=0, end_dim=1)\n",
      "      (objectness_bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "    )\n",
      "    (5): ProbObjectnessHead(\n",
      "      (flatten): Flatten(start_dim=0, end_dim=1)\n",
      "      (objectness_bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (query_embed): Embedding(100, 512)\n",
      "  (input_proj): ModuleList(\n",
      "    (0): Sequential(\n",
      "      (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
      "    )\n",
      "    (1): Sequential(\n",
      "      (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
      "    )\n",
      "    (2): Sequential(\n",
      "      (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
      "    )\n",
      "    (3): Sequential(\n",
      "      (0): Conv2d(2048, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "      (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
      "    )\n",
      "  )\n",
      "  (backbone): Joiner(\n",
      "    (0): Backbone(\n",
      "      (body): IntermediateLayerGetter(\n",
      "        (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "        (bn1): FrozenBatchNorm2d()\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "        (layer1): Sequential(\n",
      "          (0): Bottleneck(\n",
      "            (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn1): FrozenBatchNorm2d()\n",
      "            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): FrozenBatchNorm2d()\n",
      "            (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn3): FrozenBatchNorm2d()\n",
      "            (relu): ReLU(inplace=True)\n",
      "            (downsample): Sequential(\n",
      "              (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (1): FrozenBatchNorm2d()\n",
      "            )\n",
      "          )\n",
      "          (1): Bottleneck(\n",
      "            (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn1): FrozenBatchNorm2d()\n",
      "            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): FrozenBatchNorm2d()\n",
      "            (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn3): FrozenBatchNorm2d()\n",
      "            (relu): ReLU(inplace=True)\n",
      "          )\n",
      "          (2): Bottleneck(\n",
      "            (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn1): FrozenBatchNorm2d()\n",
      "            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): FrozenBatchNorm2d()\n",
      "            (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn3): FrozenBatchNorm2d()\n",
      "            (relu): ReLU(inplace=True)\n",
      "          )\n",
      "        )\n",
      "        (layer2): Sequential(\n",
      "          (0): Bottleneck(\n",
      "            (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn1): FrozenBatchNorm2d()\n",
      "            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): FrozenBatchNorm2d()\n",
      "            (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn3): FrozenBatchNorm2d()\n",
      "            (relu): ReLU(inplace=True)\n",
      "            (downsample): Sequential(\n",
      "              (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "              (1): FrozenBatchNorm2d()\n",
      "            )\n",
      "          )\n",
      "          (1): Bottleneck(\n",
      "            (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn1): FrozenBatchNorm2d()\n",
      "            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): FrozenBatchNorm2d()\n",
      "            (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn3): FrozenBatchNorm2d()\n",
      "            (relu): ReLU(inplace=True)\n",
      "          )\n",
      "          (2): Bottleneck(\n",
      "            (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn1): FrozenBatchNorm2d()\n",
      "            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): FrozenBatchNorm2d()\n",
      "            (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn3): FrozenBatchNorm2d()\n",
      "            (relu): ReLU(inplace=True)\n",
      "          )\n",
      "          (3): Bottleneck(\n",
      "            (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn1): FrozenBatchNorm2d()\n",
      "            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): FrozenBatchNorm2d()\n",
      "            (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn3): FrozenBatchNorm2d()\n",
      "            (relu): ReLU(inplace=True)\n",
      "          )\n",
      "        )\n",
      "        (layer3): Sequential(\n",
      "          (0): Bottleneck(\n",
      "            (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn1): FrozenBatchNorm2d()\n",
      "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): FrozenBatchNorm2d()\n",
      "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn3): FrozenBatchNorm2d()\n",
      "            (relu): ReLU(inplace=True)\n",
      "            (downsample): Sequential(\n",
      "              (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "              (1): FrozenBatchNorm2d()\n",
      "            )\n",
      "          )\n",
      "          (1): Bottleneck(\n",
      "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn1): FrozenBatchNorm2d()\n",
      "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): FrozenBatchNorm2d()\n",
      "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn3): FrozenBatchNorm2d()\n",
      "            (relu): ReLU(inplace=True)\n",
      "          )\n",
      "          (2): Bottleneck(\n",
      "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn1): FrozenBatchNorm2d()\n",
      "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): FrozenBatchNorm2d()\n",
      "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn3): FrozenBatchNorm2d()\n",
      "            (relu): ReLU(inplace=True)\n",
      "          )\n",
      "          (3): Bottleneck(\n",
      "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn1): FrozenBatchNorm2d()\n",
      "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): FrozenBatchNorm2d()\n",
      "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn3): FrozenBatchNorm2d()\n",
      "            (relu): ReLU(inplace=True)\n",
      "          )\n",
      "          (4): Bottleneck(\n",
      "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn1): FrozenBatchNorm2d()\n",
      "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): FrozenBatchNorm2d()\n",
      "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn3): FrozenBatchNorm2d()\n",
      "            (relu): ReLU(inplace=True)\n",
      "          )\n",
      "          (5): Bottleneck(\n",
      "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn1): FrozenBatchNorm2d()\n",
      "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): FrozenBatchNorm2d()\n",
      "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn3): FrozenBatchNorm2d()\n",
      "            (relu): ReLU(inplace=True)\n",
      "          )\n",
      "        )\n",
      "        (layer4): Sequential(\n",
      "          (0): Bottleneck(\n",
      "            (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn1): FrozenBatchNorm2d()\n",
      "            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): FrozenBatchNorm2d()\n",
      "            (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn3): FrozenBatchNorm2d()\n",
      "            (relu): ReLU(inplace=True)\n",
      "            (downsample): Sequential(\n",
      "              (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "              (1): FrozenBatchNorm2d()\n",
      "            )\n",
      "          )\n",
      "          (1): Bottleneck(\n",
      "            (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn1): FrozenBatchNorm2d()\n",
      "            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): FrozenBatchNorm2d()\n",
      "            (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn3): FrozenBatchNorm2d()\n",
      "            (relu): ReLU(inplace=True)\n",
      "          )\n",
      "          (2): Bottleneck(\n",
      "            (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn1): FrozenBatchNorm2d()\n",
      "            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): FrozenBatchNorm2d()\n",
      "            (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn3): FrozenBatchNorm2d()\n",
      "            (relu): ReLU(inplace=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1): PositionEmbeddingSine()\n",
      "  )\n",
      ")\n",
      "number of params: 39742295\n",
      "TOWOD\n",
      "owod_t1_toy_5classes_train\n",
      "owod_all_task_test\n",
      "Dataset OWDetection\n",
      "    Number of datapoints: 214\n",
      "    Root location: ./data/OWOD\n",
      "    [['train'], Compose(\n",
      "    <datasets.transforms.RandomHorizontalFlip object at 0x780c80483fa0>\n",
      "    <datasets.transforms.RandomSelect object at 0x780c80482560>\n",
      "    Compose(\n",
      "    <datasets.transforms.ToTensor object at 0x780c804802b0>\n",
      "    <datasets.transforms.Normalize object at 0x780c80483b20>\n",
      ")\n",
      ")]\n",
      "Dataset OWDetection\n",
      "    Number of datapoints: 10246\n",
      "    Root location: ./data/OWOD\n",
      "    [['test'], Compose(\n",
      "    <datasets.transforms.RandomResize object at 0x780b579e1120>\n",
      "    Compose(\n",
      "    <datasets.transforms.ToTensor object at 0x780b579f42b0>\n",
      "    <datasets.transforms.Normalize object at 0x780b579f6560>\n",
      ")\n",
      ")]\n"
     ]
    }
   ],
   "source": [
    "# before train: experimental setting (no ddp setting)\n",
    "# main() in main_open_world.py\n",
    "\n",
    "if args.frozen_weights is not None:\n",
    "        assert args.masks, \"Frozen training is meant for segmentation only\"\n",
    "print(args)\n",
    "\n",
    "device = torch.device(args.device)\n",
    "\n",
    "# fix the seed for reproducibility\n",
    "seed = args.seed\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "\n",
    "model, criterion, postprocessors, exemplar_selection = build_model(args, mode = args.model_type)\n",
    "model.to(device)\n",
    "\n",
    "model_without_ddp = model\n",
    "print(model_without_ddp)\n",
    "n_parameters = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print('number of params:', n_parameters)\n",
    "\n",
    "dataset_train, dataset_val = get_datasets(args)\n",
    "\n",
    "sampler_train = torch.utils.data.RandomSampler(dataset_train)\n",
    "sampler_val = torch.utils.data.SequentialSampler(dataset_val)\n",
    "\n",
    "batch_sampler_train = torch.utils.data.BatchSampler(sampler_train, args.batch_size, drop_last=True)\n",
    "data_loader_train = DataLoader(dataset_train, batch_sampler=batch_sampler_train,\n",
    "                                collate_fn=utils.collate_fn, num_workers=args.num_workers,\n",
    "                                pin_memory=True)\n",
    "data_loader_val = DataLoader(dataset_val, args.batch_size, sampler=sampler_val,\n",
    "                                drop_last=False, collate_fn=utils.collate_fn, num_workers=args.num_workers,\n",
    "                                pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lr_backbone_names = [\"backbone.0\", \"backbone.neck\", \"input_proj\", \"transformer.encoder\"]\n",
    "def match_name_keywords(n, name_keywords):\n",
    "    out = False\n",
    "    for b in name_keywords:\n",
    "        if b in n:\n",
    "            out = True\n",
    "            break\n",
    "    return out\n",
    "\n",
    "param_dicts = [\n",
    "    {\n",
    "        \"params\":\n",
    "            [p for n, p in model_without_ddp.named_parameters()\n",
    "                if not match_name_keywords(n, args.lr_backbone_names) and not match_name_keywords(n, args.lr_linear_proj_names) and p.requires_grad],\n",
    "        \"lr\": args.lr,\n",
    "    },\n",
    "    {\n",
    "        \"params\": [p for n, p in model_without_ddp.named_parameters() if match_name_keywords(n, args.lr_backbone_names) and p.requires_grad],\n",
    "        \"lr\": args.lr_backbone,\n",
    "    },\n",
    "    {\n",
    "        \"params\": [p for n, p in model_without_ddp.named_parameters() if match_name_keywords(n, args.lr_linear_proj_names) and p.requires_grad],\n",
    "        \"lr\": args.lr * args.lr_linear_proj_mult,\n",
    "    }\n",
    "]\n",
    "if args.sgd:\n",
    "    optimizer = torch.optim.SGD(param_dicts, lr=args.lr, momentum=0.9,\n",
    "                                weight_decay=args.weight_decay)\n",
    "else:\n",
    "    optimizer = torch.optim.AdamW(param_dicts, lr=args.lr,\n",
    "                                    weight_decay=args.weight_decay)\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, args.lr_drop)\n",
    "\n",
    "# removed \"coco_panoptic\" and \"coco\" branch\n",
    "base_ds = dataset_val\n",
    "\n",
    "if args.frozen_weights is not None:\n",
    "    checkpoint = torch.load(args.frozen_weights, map_location='cpu')\n",
    "    model_without_ddp.detr.load_state_dict(checkpoint['model'])\n",
    "    \n",
    "output_dir = Path(args.output_dir)\n",
    "\n",
    "if args.pretrain:\n",
    "    print('Initialized from the pre-training model')\n",
    "    checkpoint = torch.load(args.pretrain, map_location='cpu')\n",
    "    state_dict = checkpoint['model']\n",
    "    msg = model_without_ddp.load_state_dict(state_dict, strict=False)\n",
    "    print(msg)\n",
    "    args.start_epoch = checkpoint['epoch'] + 1\n",
    "    if args.eval: \n",
    "        test_stats, coco_evaluator = evaluate(model, criterion, postprocessors, data_loader_val, base_ds, device, args.output_dir, args)\n",
    "        # return\n",
    "        \n",
    "if args.freeze_prob_model:           \n",
    "    if isinstance(model_without_ddp.prob_obj_head, torch.nn.ModuleList):\n",
    "        for obj_head in model_without_ddp.prob_obj_head:\n",
    "            obj_head.freeze_prob_model()\n",
    "    else:\n",
    "        model_without_ddp.prob_obj_head.freeze_prob_model()\n",
    "        \n",
    "    obj_bn_mean_before=model_without_ddp.prob_obj_head[0].objectness_bn.running_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DotMap(_ipython_display_=DotMap(), _repr_mimebundle_=DotMap())"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# training\n",
    "\n",
    "print(f'Start training from epoch {args.start_epoch} to {args.epochs}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
