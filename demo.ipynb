{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/.local/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'OWDETR': ('aeroplane', 'bicycle', 'bird', 'boat', 'bus', 'car', 'cat', 'cow', 'dog', 'horse', 'motorbike', 'sheep', 'train', 'elephant', 'bear', 'zebra', 'giraffe', 'truck', 'person', 'traffic light', 'fire hydrant', 'stop sign', 'parking meter', 'bench', 'chair', 'diningtable', 'pottedplant', 'backpack', 'umbrella', 'handbag', 'tie', 'suitcase', 'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'bed', 'toilet', 'sofa', 'frisbee', 'skis', 'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket', 'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone', 'book', 'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'tvmonitor', 'bottle', 'unknown'), 'TOWOD': ('aeroplane', 'bicycle', 'bird', 'boat', 'bottle', 'bus', 'car', 'cat', 'chair', 'cow', 'diningtable', 'dog', 'horse', 'motorbike', 'person', 'pottedplant', 'sheep', 'sofa', 'train', 'tvmonitor', 'truck', 'traffic light', 'fire hydrant', 'stop sign', 'parking meter', 'bench', 'elephant', 'bear', 'zebra', 'giraffe', 'backpack', 'umbrella', 'handbag', 'tie', 'suitcase', 'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'frisbee', 'skis', 'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket', 'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'bed', 'toilet', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone', 'book', 'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'unknown'), 'VOC2007': ('aeroplane', 'bicycle', 'bird', 'boat', 'bottle', 'bus', 'car', 'cat', 'chair', 'cow', 'diningtable', 'dog', 'horse', 'motorbike', 'person', 'pottedplant', 'sheep', 'sofa', 'train', 'tvmonitor', 'truck', 'traffic light', 'fire hydrant', 'stop sign', 'parking meter', 'bench', 'elephant', 'bear', 'zebra', 'giraffe', 'backpack', 'umbrella', 'handbag', 'tie', 'suitcase', 'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'frisbee', 'skis', 'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket', 'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'bed', 'toilet', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone', 'book', 'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'unknown')}\n",
      "('aeroplane', 'bicycle', 'bird', 'boat', 'bus', 'car', 'cat', 'cow', 'dog', 'horse', 'motorbike', 'sheep', 'train', 'elephant', 'bear', 'zebra', 'giraffe', 'truck', 'person', 'traffic light', 'fire hydrant', 'stop sign', 'parking meter', 'bench', 'chair', 'diningtable', 'pottedplant', 'backpack', 'umbrella', 'handbag', 'tie', 'suitcase', 'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'bed', 'toilet', 'sofa', 'frisbee', 'skis', 'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket', 'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone', 'book', 'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'tvmonitor', 'bottle', 'unknown')\n",
      "Namespace(lr=0.0002, lr_backbone_names=['backbone.0'], lr_backbone=2e-05, lr_linear_proj_names=['reference_points', 'sampling_offsets'], lr_linear_proj_mult=0.1, batch_size=2, weight_decay=0.0001, epochs=1, lr_drop=35, lr_drop_epochs=None, clip_max_norm=0.1, sgd=False, with_box_refine=False, two_stage=False, masks=False, backbone='dino_resnet50', frozen_weights=None, dilation=False, position_embedding='sine', position_embedding_scale=6.283185307179586, num_feature_levels=4, enc_layers=6, dec_layers=6, dim_feedforward=1024, hidden_dim=256, dropout=0.1, nheads=8, num_queries=100, dec_n_points=4, enc_n_points=4, aux_loss=True, set_cost_class=2, set_cost_bbox=5, set_cost_giou=2, cls_loss_coef=2, bbox_loss_coef=5, giou_loss_coef=2, focal_alpha=0.25, coco_panoptic_path=None, remove_difficult=False, output_dir='', device='cuda', seed=42, resume='', start_epoch=0, eval=False, viz=False, eval_every=5, num_workers=3, cache_mode=False, PREV_INTRODUCED_CLS=0, CUR_INTRODUCED_CLS=20, unmatched_boxes=False, top_unk=5, featdim=1024, invalid_cls_logits=False, NC_branch=False, bbox_thresh=0.3, pretrain='', nc_loss_coef=2, train_set='', test_set='', num_classes=81, nc_epoch=0, dataset='OWDETR', data_root='./data/OWOD', unk_conf_w=1.0, model_type='prob', wandb_name='', wandb_project='', obj_loss_coef=1, obj_temp=1, freeze_prob_model=False, num_inst_per_class=50, exemplar_replay_selection=False, exemplar_replay_max_length=10000000000.0, exemplar_replay_dir='', exemplar_replay_prev_file='', exemplar_replay_cur_file='', exemplar_replay_random=False)\n"
     ]
    }
   ],
   "source": [
    "# preparation for demo, use default settings(args) \n",
    "import argparse\n",
    "\n",
    "from main_open_world import get_args_parser\n",
    "\n",
    "parser = argparse.ArgumentParser('Deformable DETR training and evaluation script', parents=[get_args_parser()])\n",
    "\n",
    "args = parser.parse_args(args = []) # args = [] : fix the error https://chaeso-coding.tistory.com/113\n",
    "args.epochs = 1\n",
    "print(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid class range: [20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79]\n",
      "DINO resnet50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running with exemplar_replay_selection\n",
      "DeformableDETR(\n",
      "  (transformer): DeformableTransformer(\n",
      "    (encoder): DeformableTransformerEncoder(\n",
      "      (layers): ModuleList(\n",
      "        (0): DeformableTransformerEncoderLayer(\n",
      "          (self_attn): MSDeformAttn(\n",
      "            (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (attention_weights): Linear(in_features=256, out_features=128, bias=True)\n",
      "            (value_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (output_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "          )\n",
      "          (dropout1): Dropout(p=0.1, inplace=False)\n",
      "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (linear1): Linear(in_features=256, out_features=1024, bias=True)\n",
      "          (dropout2): Dropout(p=0.1, inplace=False)\n",
      "          (linear2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "          (dropout3): Dropout(p=0.1, inplace=False)\n",
      "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (1): DeformableTransformerEncoderLayer(\n",
      "          (self_attn): MSDeformAttn(\n",
      "            (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (attention_weights): Linear(in_features=256, out_features=128, bias=True)\n",
      "            (value_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (output_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "          )\n",
      "          (dropout1): Dropout(p=0.1, inplace=False)\n",
      "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (linear1): Linear(in_features=256, out_features=1024, bias=True)\n",
      "          (dropout2): Dropout(p=0.1, inplace=False)\n",
      "          (linear2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "          (dropout3): Dropout(p=0.1, inplace=False)\n",
      "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (2): DeformableTransformerEncoderLayer(\n",
      "          (self_attn): MSDeformAttn(\n",
      "            (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (attention_weights): Linear(in_features=256, out_features=128, bias=True)\n",
      "            (value_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (output_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "          )\n",
      "          (dropout1): Dropout(p=0.1, inplace=False)\n",
      "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (linear1): Linear(in_features=256, out_features=1024, bias=True)\n",
      "          (dropout2): Dropout(p=0.1, inplace=False)\n",
      "          (linear2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "          (dropout3): Dropout(p=0.1, inplace=False)\n",
      "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (3): DeformableTransformerEncoderLayer(\n",
      "          (self_attn): MSDeformAttn(\n",
      "            (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (attention_weights): Linear(in_features=256, out_features=128, bias=True)\n",
      "            (value_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (output_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "          )\n",
      "          (dropout1): Dropout(p=0.1, inplace=False)\n",
      "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (linear1): Linear(in_features=256, out_features=1024, bias=True)\n",
      "          (dropout2): Dropout(p=0.1, inplace=False)\n",
      "          (linear2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "          (dropout3): Dropout(p=0.1, inplace=False)\n",
      "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (4): DeformableTransformerEncoderLayer(\n",
      "          (self_attn): MSDeformAttn(\n",
      "            (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (attention_weights): Linear(in_features=256, out_features=128, bias=True)\n",
      "            (value_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (output_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "          )\n",
      "          (dropout1): Dropout(p=0.1, inplace=False)\n",
      "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (linear1): Linear(in_features=256, out_features=1024, bias=True)\n",
      "          (dropout2): Dropout(p=0.1, inplace=False)\n",
      "          (linear2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "          (dropout3): Dropout(p=0.1, inplace=False)\n",
      "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (5): DeformableTransformerEncoderLayer(\n",
      "          (self_attn): MSDeformAttn(\n",
      "            (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (attention_weights): Linear(in_features=256, out_features=128, bias=True)\n",
      "            (value_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (output_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "          )\n",
      "          (dropout1): Dropout(p=0.1, inplace=False)\n",
      "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (linear1): Linear(in_features=256, out_features=1024, bias=True)\n",
      "          (dropout2): Dropout(p=0.1, inplace=False)\n",
      "          (linear2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "          (dropout3): Dropout(p=0.1, inplace=False)\n",
      "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (decoder): DeformableTransformerDecoder(\n",
      "      (layers): ModuleList(\n",
      "        (0): DeformableTransformerDecoderLayer(\n",
      "          (cross_attn): MSDeformAttn(\n",
      "            (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (attention_weights): Linear(in_features=256, out_features=128, bias=True)\n",
      "            (value_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (output_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "          )\n",
      "          (dropout1): Dropout(p=0.1, inplace=False)\n",
      "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
      "          )\n",
      "          (dropout2): Dropout(p=0.1, inplace=False)\n",
      "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (linear1): Linear(in_features=256, out_features=1024, bias=True)\n",
      "          (dropout3): Dropout(p=0.1, inplace=False)\n",
      "          (linear2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "          (dropout4): Dropout(p=0.1, inplace=False)\n",
      "          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (1): DeformableTransformerDecoderLayer(\n",
      "          (cross_attn): MSDeformAttn(\n",
      "            (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (attention_weights): Linear(in_features=256, out_features=128, bias=True)\n",
      "            (value_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (output_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "          )\n",
      "          (dropout1): Dropout(p=0.1, inplace=False)\n",
      "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
      "          )\n",
      "          (dropout2): Dropout(p=0.1, inplace=False)\n",
      "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (linear1): Linear(in_features=256, out_features=1024, bias=True)\n",
      "          (dropout3): Dropout(p=0.1, inplace=False)\n",
      "          (linear2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "          (dropout4): Dropout(p=0.1, inplace=False)\n",
      "          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (2): DeformableTransformerDecoderLayer(\n",
      "          (cross_attn): MSDeformAttn(\n",
      "            (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (attention_weights): Linear(in_features=256, out_features=128, bias=True)\n",
      "            (value_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (output_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "          )\n",
      "          (dropout1): Dropout(p=0.1, inplace=False)\n",
      "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
      "          )\n",
      "          (dropout2): Dropout(p=0.1, inplace=False)\n",
      "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (linear1): Linear(in_features=256, out_features=1024, bias=True)\n",
      "          (dropout3): Dropout(p=0.1, inplace=False)\n",
      "          (linear2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "          (dropout4): Dropout(p=0.1, inplace=False)\n",
      "          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (3): DeformableTransformerDecoderLayer(\n",
      "          (cross_attn): MSDeformAttn(\n",
      "            (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (attention_weights): Linear(in_features=256, out_features=128, bias=True)\n",
      "            (value_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (output_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "          )\n",
      "          (dropout1): Dropout(p=0.1, inplace=False)\n",
      "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
      "          )\n",
      "          (dropout2): Dropout(p=0.1, inplace=False)\n",
      "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (linear1): Linear(in_features=256, out_features=1024, bias=True)\n",
      "          (dropout3): Dropout(p=0.1, inplace=False)\n",
      "          (linear2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "          (dropout4): Dropout(p=0.1, inplace=False)\n",
      "          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (4): DeformableTransformerDecoderLayer(\n",
      "          (cross_attn): MSDeformAttn(\n",
      "            (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (attention_weights): Linear(in_features=256, out_features=128, bias=True)\n",
      "            (value_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (output_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "          )\n",
      "          (dropout1): Dropout(p=0.1, inplace=False)\n",
      "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
      "          )\n",
      "          (dropout2): Dropout(p=0.1, inplace=False)\n",
      "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (linear1): Linear(in_features=256, out_features=1024, bias=True)\n",
      "          (dropout3): Dropout(p=0.1, inplace=False)\n",
      "          (linear2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "          (dropout4): Dropout(p=0.1, inplace=False)\n",
      "          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (5): DeformableTransformerDecoderLayer(\n",
      "          (cross_attn): MSDeformAttn(\n",
      "            (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (attention_weights): Linear(in_features=256, out_features=128, bias=True)\n",
      "            (value_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (output_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "          )\n",
      "          (dropout1): Dropout(p=0.1, inplace=False)\n",
      "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
      "          )\n",
      "          (dropout2): Dropout(p=0.1, inplace=False)\n",
      "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (linear1): Linear(in_features=256, out_features=1024, bias=True)\n",
      "          (dropout3): Dropout(p=0.1, inplace=False)\n",
      "          (linear2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "          (dropout4): Dropout(p=0.1, inplace=False)\n",
      "          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (reference_points): Linear(in_features=256, out_features=2, bias=True)\n",
      "  )\n",
      "  (class_embed): ModuleList(\n",
      "    (0): Linear(in_features=256, out_features=81, bias=True)\n",
      "    (1): Linear(in_features=256, out_features=81, bias=True)\n",
      "    (2): Linear(in_features=256, out_features=81, bias=True)\n",
      "    (3): Linear(in_features=256, out_features=81, bias=True)\n",
      "    (4): Linear(in_features=256, out_features=81, bias=True)\n",
      "    (5): Linear(in_features=256, out_features=81, bias=True)\n",
      "  )\n",
      "  (bbox_embed): ModuleList(\n",
      "    (0): MLP(\n",
      "      (layers): ModuleList(\n",
      "        (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "        (1): Linear(in_features=256, out_features=256, bias=True)\n",
      "        (2): Linear(in_features=256, out_features=4, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (1): MLP(\n",
      "      (layers): ModuleList(\n",
      "        (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "        (1): Linear(in_features=256, out_features=256, bias=True)\n",
      "        (2): Linear(in_features=256, out_features=4, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (2): MLP(\n",
      "      (layers): ModuleList(\n",
      "        (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "        (1): Linear(in_features=256, out_features=256, bias=True)\n",
      "        (2): Linear(in_features=256, out_features=4, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (3): MLP(\n",
      "      (layers): ModuleList(\n",
      "        (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "        (1): Linear(in_features=256, out_features=256, bias=True)\n",
      "        (2): Linear(in_features=256, out_features=4, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (4): MLP(\n",
      "      (layers): ModuleList(\n",
      "        (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "        (1): Linear(in_features=256, out_features=256, bias=True)\n",
      "        (2): Linear(in_features=256, out_features=4, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (5): MLP(\n",
      "      (layers): ModuleList(\n",
      "        (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "        (1): Linear(in_features=256, out_features=256, bias=True)\n",
      "        (2): Linear(in_features=256, out_features=4, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (prob_obj_head): ModuleList(\n",
      "    (0): ProbObjectnessHead(\n",
      "      (flatten): Flatten(start_dim=0, end_dim=1)\n",
      "      (objectness_bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "    )\n",
      "    (1): ProbObjectnessHead(\n",
      "      (flatten): Flatten(start_dim=0, end_dim=1)\n",
      "      (objectness_bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "    )\n",
      "    (2): ProbObjectnessHead(\n",
      "      (flatten): Flatten(start_dim=0, end_dim=1)\n",
      "      (objectness_bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "    )\n",
      "    (3): ProbObjectnessHead(\n",
      "      (flatten): Flatten(start_dim=0, end_dim=1)\n",
      "      (objectness_bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "    )\n",
      "    (4): ProbObjectnessHead(\n",
      "      (flatten): Flatten(start_dim=0, end_dim=1)\n",
      "      (objectness_bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "    )\n",
      "    (5): ProbObjectnessHead(\n",
      "      (flatten): Flatten(start_dim=0, end_dim=1)\n",
      "      (objectness_bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (query_embed): Embedding(100, 512)\n",
      "  (input_proj): ModuleList(\n",
      "    (0): Sequential(\n",
      "      (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
      "    )\n",
      "    (1): Sequential(\n",
      "      (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
      "    )\n",
      "    (2): Sequential(\n",
      "      (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
      "    )\n",
      "    (3): Sequential(\n",
      "      (0): Conv2d(2048, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "      (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
      "    )\n",
      "  )\n",
      "  (backbone): Joiner(\n",
      "    (0): Backbone(\n",
      "      (body): IntermediateLayerGetter(\n",
      "        (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "        (bn1): FrozenBatchNorm2d()\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "        (layer1): Sequential(\n",
      "          (0): Bottleneck(\n",
      "            (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn1): FrozenBatchNorm2d()\n",
      "            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): FrozenBatchNorm2d()\n",
      "            (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn3): FrozenBatchNorm2d()\n",
      "            (relu): ReLU(inplace=True)\n",
      "            (downsample): Sequential(\n",
      "              (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (1): FrozenBatchNorm2d()\n",
      "            )\n",
      "          )\n",
      "          (1): Bottleneck(\n",
      "            (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn1): FrozenBatchNorm2d()\n",
      "            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): FrozenBatchNorm2d()\n",
      "            (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn3): FrozenBatchNorm2d()\n",
      "            (relu): ReLU(inplace=True)\n",
      "          )\n",
      "          (2): Bottleneck(\n",
      "            (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn1): FrozenBatchNorm2d()\n",
      "            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): FrozenBatchNorm2d()\n",
      "            (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn3): FrozenBatchNorm2d()\n",
      "            (relu): ReLU(inplace=True)\n",
      "          )\n",
      "        )\n",
      "        (layer2): Sequential(\n",
      "          (0): Bottleneck(\n",
      "            (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn1): FrozenBatchNorm2d()\n",
      "            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): FrozenBatchNorm2d()\n",
      "            (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn3): FrozenBatchNorm2d()\n",
      "            (relu): ReLU(inplace=True)\n",
      "            (downsample): Sequential(\n",
      "              (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "              (1): FrozenBatchNorm2d()\n",
      "            )\n",
      "          )\n",
      "          (1): Bottleneck(\n",
      "            (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn1): FrozenBatchNorm2d()\n",
      "            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): FrozenBatchNorm2d()\n",
      "            (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn3): FrozenBatchNorm2d()\n",
      "            (relu): ReLU(inplace=True)\n",
      "          )\n",
      "          (2): Bottleneck(\n",
      "            (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn1): FrozenBatchNorm2d()\n",
      "            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): FrozenBatchNorm2d()\n",
      "            (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn3): FrozenBatchNorm2d()\n",
      "            (relu): ReLU(inplace=True)\n",
      "          )\n",
      "          (3): Bottleneck(\n",
      "            (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn1): FrozenBatchNorm2d()\n",
      "            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): FrozenBatchNorm2d()\n",
      "            (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn3): FrozenBatchNorm2d()\n",
      "            (relu): ReLU(inplace=True)\n",
      "          )\n",
      "        )\n",
      "        (layer3): Sequential(\n",
      "          (0): Bottleneck(\n",
      "            (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn1): FrozenBatchNorm2d()\n",
      "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): FrozenBatchNorm2d()\n",
      "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn3): FrozenBatchNorm2d()\n",
      "            (relu): ReLU(inplace=True)\n",
      "            (downsample): Sequential(\n",
      "              (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "              (1): FrozenBatchNorm2d()\n",
      "            )\n",
      "          )\n",
      "          (1): Bottleneck(\n",
      "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn1): FrozenBatchNorm2d()\n",
      "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): FrozenBatchNorm2d()\n",
      "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn3): FrozenBatchNorm2d()\n",
      "            (relu): ReLU(inplace=True)\n",
      "          )\n",
      "          (2): Bottleneck(\n",
      "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn1): FrozenBatchNorm2d()\n",
      "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): FrozenBatchNorm2d()\n",
      "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn3): FrozenBatchNorm2d()\n",
      "            (relu): ReLU(inplace=True)\n",
      "          )\n",
      "          (3): Bottleneck(\n",
      "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn1): FrozenBatchNorm2d()\n",
      "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): FrozenBatchNorm2d()\n",
      "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn3): FrozenBatchNorm2d()\n",
      "            (relu): ReLU(inplace=True)\n",
      "          )\n",
      "          (4): Bottleneck(\n",
      "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn1): FrozenBatchNorm2d()\n",
      "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): FrozenBatchNorm2d()\n",
      "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn3): FrozenBatchNorm2d()\n",
      "            (relu): ReLU(inplace=True)\n",
      "          )\n",
      "          (5): Bottleneck(\n",
      "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn1): FrozenBatchNorm2d()\n",
      "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): FrozenBatchNorm2d()\n",
      "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn3): FrozenBatchNorm2d()\n",
      "            (relu): ReLU(inplace=True)\n",
      "          )\n",
      "        )\n",
      "        (layer4): Sequential(\n",
      "          (0): Bottleneck(\n",
      "            (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn1): FrozenBatchNorm2d()\n",
      "            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): FrozenBatchNorm2d()\n",
      "            (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn3): FrozenBatchNorm2d()\n",
      "            (relu): ReLU(inplace=True)\n",
      "            (downsample): Sequential(\n",
      "              (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "              (1): FrozenBatchNorm2d()\n",
      "            )\n",
      "          )\n",
      "          (1): Bottleneck(\n",
      "            (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn1): FrozenBatchNorm2d()\n",
      "            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): FrozenBatchNorm2d()\n",
      "            (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn3): FrozenBatchNorm2d()\n",
      "            (relu): ReLU(inplace=True)\n",
      "          )\n",
      "          (2): Bottleneck(\n",
      "            (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn1): FrozenBatchNorm2d()\n",
      "            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): FrozenBatchNorm2d()\n",
      "            (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn3): FrozenBatchNorm2d()\n",
      "            (relu): ReLU(inplace=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1): PositionEmbeddingSine()\n",
      "  )\n",
      ")\n",
      "number of params: 39742295\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "from pathlib import Path\n",
    "from models import build_model\n",
    "\n",
    "device = torch.device(args.device)\n",
    "if args.output_dir:\n",
    "        Path(args.output_dir).mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "model, criterion, postprocessors, exemplar_selection = build_model(args, mode = args.model_type)\n",
    "model.to(device) # create_model\n",
    "\n",
    "model_without_ddp = model\n",
    "print(model_without_ddp)\n",
    "n_parameters = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print('number of params:', n_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOWOD\n",
      "owod_t1_train\n",
      "owod_all_task_test\n",
      "Dataset OWDetection\n",
      "    Number of datapoints: 16551\n",
      "    Root location: ./data/OWOD\n",
      "    [['train'], Compose(\n",
      "    <datasets.transforms.RandomHorizontalFlip object at 0x7facd8ab2670>\n",
      "    <datasets.transforms.RandomSelect object at 0x7facd81b14c0>\n",
      "    Compose(\n",
      "    <datasets.transforms.ToTensor object at 0x7facd8ab26a0>\n",
      "    <datasets.transforms.Normalize object at 0x7facd8ab2d00>\n",
      ")\n",
      ")]\n",
      "Dataset OWDetection\n",
      "    Number of datapoints: 10246\n",
      "    Root location: ./data/OWOD\n",
      "    [['test'], Compose(\n",
      "    <datasets.transforms.RandomResize object at 0x7facd8c70970>\n",
      "    Compose(\n",
      "    <datasets.transforms.ToTensor object at 0x7facd9e647f0>\n",
      "    <datasets.transforms.Normalize object at 0x7facd8c70fd0>\n",
      ")\n",
      ")]\n"
     ]
    }
   ],
   "source": [
    "import util.misc as utils\n",
    "import datasets\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from datasets.torchvision_datasets.open_world import OWDetection\n",
    "from datasets.coco import make_coco_transforms\n",
    "\n",
    "\n",
    "# Ready for first task 0 (Initial training)\n",
    "\n",
    "args.train_set = 'owod_t1_train' # data/OWOD/ImageSets/TOWOD/owod_t1_train.txt\n",
    "args.test_set = 'owod_all_task_test' # data/OWOD/ImageSets/TOWOD/owod_all_task_test.txt\n",
    "args.dataset = 'TOWOD' # data/OWOD/ImageSets/TOWOD\n",
    "\n",
    "\n",
    "def get_datasets(args):\n",
    "    print(args.dataset)\n",
    "\n",
    "    train_set = args.train_set\n",
    "    test_set = args.test_set\n",
    "    dataset_train = OWDetection(args, args.data_root, image_set=args.train_set, transforms=make_coco_transforms(args.train_set), dataset = args.dataset)\n",
    "    dataset_val = OWDetection(args, args.data_root, image_set=args.test_set, dataset = args.dataset, transforms=make_coco_transforms(args.test_set))\n",
    "\n",
    "    print(args.train_set)\n",
    "    print(args.test_set)\n",
    "    print(dataset_train)\n",
    "    print(dataset_val)\n",
    "\n",
    "    return dataset_train, dataset_val\n",
    "\n",
    "\n",
    "dataset_train, dataset_val = get_datasets(args)\n",
    "\n",
    "sampler_train = torch.utils.data.RandomSampler(dataset_train)\n",
    "sampler_val = torch.utils.data.SequentialSampler(dataset_val)\n",
    "\n",
    "batch_sampler_train = torch.utils.data.BatchSampler(sampler_train, args.batch_size, drop_last=True)\n",
    "data_loader_train = DataLoader(dataset_train, batch_sampler=batch_sampler_train,\n",
    "                                collate_fn=utils.collate_fn, num_workers=args.num_workers,\n",
    "                                pin_memory=True)\n",
    "data_loader_val = DataLoader(dataset_val, args.batch_size, sampler=sampler_val,\n",
    "                                drop_last=False, collate_fn=utils.collate_fn, num_workers=args.num_workers,\n",
    "                                pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from engine import evaluate\n",
    "from datasets import build_dataset, get_coco_api_from_dataset\n",
    "\n",
    "def match_name_keywords(n, name_keywords):\n",
    "        out = False\n",
    "        for b in name_keywords:\n",
    "            if b in n:\n",
    "                out = True\n",
    "                break\n",
    "        return out\n",
    "\n",
    "model_without_ddp = model\n",
    "\n",
    "param_dicts = [\n",
    "    {\n",
    "        \"params\":\n",
    "            [p for n, p in model_without_ddp.named_parameters()\n",
    "                if not match_name_keywords(n, args.lr_backbone_names) and not match_name_keywords(n, args.lr_linear_proj_names) and p.requires_grad],\n",
    "        \"lr\": args.lr,\n",
    "    },\n",
    "    {\n",
    "        \"params\": [p for n, p in model_without_ddp.named_parameters() if match_name_keywords(n, args.lr_backbone_names) and p.requires_grad],\n",
    "        \"lr\": args.lr_backbone,\n",
    "    },\n",
    "    {\n",
    "        \"params\": [p for n, p in model_without_ddp.named_parameters() if match_name_keywords(n, args.lr_linear_proj_names) and p.requires_grad],\n",
    "        \"lr\": args.lr * args.lr_linear_proj_mult,\n",
    "    }\n",
    "]\n",
    "if args.sgd:\n",
    "    optimizer = torch.optim.SGD(param_dicts, lr=args.lr, momentum=0.9,\n",
    "                                weight_decay=args.weight_decay)\n",
    "else:\n",
    "    optimizer = torch.optim.AdamW(param_dicts, lr=args.lr,\n",
    "                                    weight_decay=args.weight_decay)\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, args.lr_drop)\n",
    "\n",
    "# if args.distributed:\n",
    "#     model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[args.gpu])\n",
    "#     model_without_ddp = model.module\n",
    "\n",
    "if args.dataset == \"coco_panoptic\":\n",
    "    # We also evaluate AP during panoptic training, on original coco DS\n",
    "    coco_val = datasets.coco.build(\"val\", args)\n",
    "    base_ds = get_coco_api_from_dataset(coco_val)\n",
    "elif args.dataset == \"coco\":\n",
    "    base_ds = get_coco_api_from_dataset(dataset_val)\n",
    "else:\n",
    "    base_ds = dataset_val\n",
    "\n",
    "if args.frozen_weights is not None:\n",
    "    checkpoint = torch.load(args.frozen_weights, map_location='cpu')\n",
    "    model_without_ddp.detr.load_state_dict(checkpoint['model'])\n",
    "\n",
    "output_dir = Path(args.output_dir)\n",
    "\n",
    "if args.pretrain:\n",
    "    print('Initialized from the pre-training model')\n",
    "    checkpoint = torch.load(args.pretrain, map_location='cpu')\n",
    "    state_dict = checkpoint['model']\n",
    "    msg = model_without_ddp.load_state_dict(state_dict, strict=False)\n",
    "    print(msg)\n",
    "    args.start_epoch = checkpoint['epoch'] + 1\n",
    "    if args.eval:\n",
    "        test_stats, coco_evaluator = evaluate(model, criterion, postprocessors, data_loader_val, base_ds, device, args.output_dir, args)\n",
    "        # return\n",
    "    \n",
    "    \n",
    "if args.resume:\n",
    "    if args.resume.startswith('https'):\n",
    "        checkpoint = torch.hub.load_state_dict_from_url(\n",
    "            args.resume, map_location='cpu', check_hash=True)\n",
    "    else:\n",
    "        checkpoint = torch.load(args.resume, map_location='cpu')\n",
    "    missing_keys, unexpected_keys = model_without_ddp.load_state_dict(checkpoint['model'], strict=False)\n",
    "    unexpected_keys = [k for k in unexpected_keys if not (k.endswith('total_params') or k.endswith('total_ops'))]\n",
    "    if len(missing_keys) > 0:\n",
    "        print('Missing Keys: {}'.format(missing_keys))\n",
    "    if len(unexpected_keys) > 0:\n",
    "        print('Unexpected Keys: {}'.format(unexpected_keys))\n",
    "    if not args.eval and 'optimizer' in checkpoint and 'lr_scheduler' in checkpoint and 'epoch' in checkpoint:\n",
    "        import copy\n",
    "        p_groups = copy.deepcopy(optimizer.param_groups)\n",
    "        optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "        for pg, pg_old in zip(optimizer.param_groups, p_groups):\n",
    "            pg['lr'] = pg_old['lr']\n",
    "            pg['initial_lr'] = pg_old['initial_lr']\n",
    "        print(optimizer.param_groups)\n",
    "        lr_scheduler.load_state_dict(checkpoint['lr_scheduler'])\n",
    "        # todo: this is a hack for doing experiment that resume from checkpoint and also modify lr scheduler (e.g., decrease lr in advance).\n",
    "        args.override_resumed_lr_drop = True\n",
    "        if args.override_resumed_lr_drop:\n",
    "            print('Warning: (hack) args.override_resumed_lr_drop is set to True, so args.lr_drop would override lr_drop in resumed lr_scheduler.')\n",
    "            lr_scheduler.step_size = args.lr_drop\n",
    "            lr_scheduler.base_lrs = list(map(lambda group: group['initial_lr'], optimizer.param_groups))\n",
    "        lr_scheduler.step(lr_scheduler.last_epoch)\n",
    "        args.start_epoch = checkpoint['epoch'] + 1\n",
    "    # check the resumed model\n",
    "    if (not args.eval and not args.viz and args.dataset in ['coco', 'voc']):\n",
    "        test_stats, coco_evaluator = evaluate(\n",
    "            model, criterion, postprocessors, data_loader_val, base_ds, device, args.output_dir, args\n",
    "        )\n",
    "    if args.eval:\n",
    "        test_stats, coco_evaluator = evaluate(model, criterion, postprocessors, data_loader_val, base_ds, device, args.output_dir, args)\n",
    "        if args.output_dir:\n",
    "            utils.save_on_master(coco_evaluator.coco_eval[\"bbox\"].eval, output_dir / \"eval.pth\")\n",
    "        # return\n",
    "    \n",
    "if args.freeze_prob_model:           \n",
    "    if isinstance(model_without_ddp.prob_obj_head, torch.nn.ModuleList):\n",
    "        for obj_head in model_without_ddp.prob_obj_head:\n",
    "            obj_head.freeze_prob_model()\n",
    "    else:\n",
    "        model_without_ddp.prob_obj_head.freeze_prob_model()\n",
    "        \n",
    "    obj_bn_mean_before=model_without_ddp.prob_obj_head[0].objectness_bn.running_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training from epoch 0 to 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /opt/conda/conda-bld/pytorch_1666643016022/work/aten/src/ATen/native/TensorShape.cpp:3190.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [0]  [   0/8275]  eta: 1:53:44  lr: 0.000200  class_error: 100.00  grad_norm: 4508.54  loss: 1460.6351 (1460.6351)  loss_ce: 7.9030 (7.9030)  loss_bbox: 3.0421 (3.0421)  loss_giou: 1.7519 (1.7519)  loss_obj_ll: 218.6358 (218.6358)  loss_ce_0: 9.1670 (9.1670)  loss_bbox_0: 3.2134 (3.2134)  loss_giou_0: 1.7638 (1.7638)  loss_obj_ll_0: 233.6367 (233.6367)  loss_ce_1: 8.0880 (8.0880)  loss_bbox_1: 3.1006 (3.1006)  loss_giou_1: 1.7519 (1.7519)  loss_obj_ll_1: 239.9271 (239.9271)  loss_ce_2: 8.0075 (8.0075)  loss_bbox_2: 3.1572 (3.1572)  loss_giou_2: 1.7638 (1.7638)  loss_obj_ll_2: 236.5669 (236.5669)  loss_ce_3: 8.9035 (8.9035)  loss_bbox_3: 3.1055 (3.1055)  loss_giou_3: 1.7519 (1.7519)  loss_obj_ll_3: 222.1502 (222.1502)  loss_ce_4: 8.7117 (8.7117)  loss_bbox_4: 3.1055 (3.1055)  loss_giou_4: 1.7519 (1.7519)  loss_obj_ll_4: 229.6781 (229.6781)  loss_ce_unscaled: 3.9515 (3.9515)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.6084 (0.6084)  loss_giou_unscaled: 0.8759 (0.8759)  cardinality_error_unscaled: 82.5000 (82.5000)  loss_obj_ll_unscaled: 218.6358 (218.6358)  loss_ce_0_unscaled: 4.5835 (4.5835)  loss_bbox_0_unscaled: 0.6427 (0.6427)  loss_giou_0_unscaled: 0.8819 (0.8819)  cardinality_error_0_unscaled: 95.0000 (95.0000)  loss_obj_ll_0_unscaled: 233.6367 (233.6367)  loss_ce_1_unscaled: 4.0440 (4.0440)  loss_bbox_1_unscaled: 0.6201 (0.6201)  loss_giou_1_unscaled: 0.8759 (0.8759)  cardinality_error_1_unscaled: 85.0000 (85.0000)  loss_obj_ll_1_unscaled: 239.9271 (239.9271)  loss_ce_2_unscaled: 4.0038 (4.0038)  loss_bbox_2_unscaled: 0.6314 (0.6314)  loss_giou_2_unscaled: 0.8819 (0.8819)  cardinality_error_2_unscaled: 77.5000 (77.5000)  loss_obj_ll_2_unscaled: 236.5669 (236.5669)  loss_ce_3_unscaled: 4.4518 (4.4518)  loss_bbox_3_unscaled: 0.6211 (0.6211)  loss_giou_3_unscaled: 0.8759 (0.8759)  cardinality_error_3_unscaled: 95.0000 (95.0000)  loss_obj_ll_3_unscaled: 222.1502 (222.1502)  loss_ce_4_unscaled: 4.3558 (4.3558)  loss_bbox_4_unscaled: 0.6211 (0.6211)  loss_giou_4_unscaled: 0.8759 (0.8759)  cardinality_error_4_unscaled: 95.5000 (95.5000)  loss_obj_ll_4_unscaled: 229.6781 (229.6781)  time: 0.8247  data: 0.0000  max mem: 3783\n",
      "Epoch: [0]  [  10/8275]  eta: 0:26:40  lr: 0.000200  class_error: 87.50  grad_norm: 3100.88  loss: 1394.3136 (1371.1237)  loss_ce: 8.3870 (8.9901)  loss_bbox: 4.3836 (4.5586)  loss_giou: 1.8574 (1.8595)  loss_obj_ll: 211.7519 (208.8910)  loss_ce_0: 10.2152 (11.2228)  loss_bbox_0: 4.3474 (4.6100)  loss_giou_0: 1.8610 (1.8598)  loss_obj_ll_0: 226.2960 (225.6148)  loss_ce_1: 8.2001 (9.2015)  loss_bbox_1: 4.4000 (4.5709)  loss_giou_1: 1.8619 (1.8577)  loss_obj_ll_1: 208.3319 (210.6858)  loss_ce_2: 8.3264 (8.8980)  loss_bbox_2: 4.3450 (4.5653)  loss_giou_2: 1.8618 (1.8597)  loss_obj_ll_2: 211.8148 (207.8008)  loss_ce_3: 8.9035 (9.8632)  loss_bbox_3: 4.3555 (4.5560)  loss_giou_3: 1.8536 (1.8548)  loss_obj_ll_3: 213.1435 (210.4739)  loss_ce_4: 8.6132 (8.9309)  loss_bbox_4: 4.3609 (4.5585)  loss_giou_4: 1.8532 (1.8551)  loss_obj_ll_4: 211.1063 (211.9850)  loss_ce_unscaled: 4.1935 (4.4951)  class_error_unscaled: 100.0000 (98.8636)  loss_bbox_unscaled: 0.8767 (0.9117)  loss_giou_unscaled: 0.9287 (0.9297)  cardinality_error_unscaled: 36.5000 (46.7273)  loss_obj_ll_unscaled: 211.7519 (208.8910)  loss_ce_0_unscaled: 5.1076 (5.6114)  loss_bbox_0_unscaled: 0.8695 (0.9220)  loss_giou_0_unscaled: 0.9305 (0.9299)  cardinality_error_0_unscaled: 93.0000 (92.4545)  loss_obj_ll_0_unscaled: 226.2960 (225.6148)  loss_ce_1_unscaled: 4.1000 (4.6007)  loss_bbox_1_unscaled: 0.8800 (0.9142)  loss_giou_1_unscaled: 0.9310 (0.9289)  cardinality_error_1_unscaled: 70.0000 (66.9091)  loss_obj_ll_1_unscaled: 208.3319 (210.6858)  loss_ce_2_unscaled: 4.1632 (4.4490)  loss_bbox_2_unscaled: 0.8690 (0.9131)  loss_giou_2_unscaled: 0.9309 (0.9299)  cardinality_error_2_unscaled: 31.0000 (30.8182)  loss_obj_ll_2_unscaled: 211.8148 (207.8008)  loss_ce_3_unscaled: 4.4518 (4.9316)  loss_bbox_3_unscaled: 0.8711 (0.9112)  loss_giou_3_unscaled: 0.9268 (0.9274)  cardinality_error_3_unscaled: 84.5000 (73.3182)  loss_obj_ll_3_unscaled: 213.1435 (210.4739)  loss_ce_4_unscaled: 4.3066 (4.4654)  loss_bbox_4_unscaled: 0.8722 (0.9117)  loss_giou_4_unscaled: 0.9266 (0.9276)  cardinality_error_4_unscaled: 65.0000 (57.2727)  loss_obj_ll_4_unscaled: 211.1063 (211.9850)  time: 0.1937  data: 0.0000  max mem: 5861\n",
      "Epoch: [0]  [  20/8275]  eta: 0:21:43  lr: 0.000200  class_error: 100.00  grad_norm: 2619.04  loss: 1243.1393 (1220.3612)  loss_ce: 7.2984 (7.8159)  loss_bbox: 4.2918 (4.1839)  loss_giou: 1.8574 (1.8323)  loss_obj_ll: 182.3539 (184.2851)  loss_ce_0: 9.9832 (10.1030)  loss_bbox_0: 4.2910 (4.2069)  loss_giou_0: 1.8538 (1.8296)  loss_obj_ll_0: 216.9568 (207.5653)  loss_ce_1: 6.5595 (7.6123)  loss_bbox_1: 4.2920 (4.1733)  loss_giou_1: 1.8563 (1.8265)  loss_obj_ll_1: 185.3197 (185.9420)  loss_ce_2: 5.5498 (7.1059)  loss_bbox_2: 4.2811 (4.1595)  loss_giou_2: 1.8504 (1.8251)  loss_obj_ll_2: 190.4123 (183.5793)  loss_ce_3: 6.7360 (8.0536)  loss_bbox_3: 4.2450 (4.1611)  loss_giou_3: 1.8536 (1.8300)  loss_obj_ll_3: 191.6494 (189.1490)  loss_ce_4: 6.9551 (7.7909)  loss_bbox_4: 4.3098 (4.1765)  loss_giou_4: 1.8532 (1.8303)  loss_obj_ll_4: 191.4838 (185.3238)  loss_ce_unscaled: 3.6492 (3.9080)  class_error_unscaled: 100.0000 (97.9437)  loss_bbox_unscaled: 0.8584 (0.8368)  loss_giou_unscaled: 0.9287 (0.9161)  cardinality_error_unscaled: 20.0000 (32.0714)  loss_obj_ll_unscaled: 182.3539 (184.2851)  loss_ce_0_unscaled: 4.9916 (5.0515)  loss_bbox_0_unscaled: 0.8582 (0.8414)  loss_giou_0_unscaled: 0.9269 (0.9148)  cardinality_error_0_unscaled: 85.0000 (79.2619)  loss_obj_ll_0_unscaled: 216.9568 (207.5653)  loss_ce_1_unscaled: 3.2797 (3.8062)  loss_bbox_1_unscaled: 0.8584 (0.8347)  loss_giou_1_unscaled: 0.9282 (0.9133)  cardinality_error_1_unscaled: 26.5000 (38.3333)  loss_obj_ll_1_unscaled: 185.3197 (185.9420)  loss_ce_2_unscaled: 2.7749 (3.5529)  loss_bbox_2_unscaled: 0.8562 (0.8319)  loss_giou_2_unscaled: 0.9252 (0.9126)  cardinality_error_2_unscaled: 9.0000 (18.0000)  loss_obj_ll_2_unscaled: 190.4123 (183.5793)  loss_ce_3_unscaled: 3.3680 (4.0268)  loss_bbox_3_unscaled: 0.8490 (0.8322)  loss_giou_3_unscaled: 0.9268 (0.9150)  cardinality_error_3_unscaled: 11.5000 (41.0952)  loss_obj_ll_3_unscaled: 191.6494 (189.1490)  loss_ce_4_unscaled: 3.4776 (3.8955)  loss_bbox_4_unscaled: 0.8620 (0.8353)  loss_giou_4_unscaled: 0.9266 (0.9151)  cardinality_error_4_unscaled: 10.5000 (34.0476)  loss_obj_ll_4_unscaled: 191.4838 (185.3238)  time: 0.1245  data: 0.0000  max mem: 5861\n",
      "Epoch: [0]  [  30/8275]  eta: 0:19:31  lr: 0.000200  class_error: 100.00  grad_norm: 3405.66  loss: 801.5898 (1137.1281)  loss_ce: 5.1934 (7.3558)  loss_bbox: 3.2558 (4.0425)  loss_giou: 1.7345 (1.8010)  loss_obj_ll: 114.9613 (172.7754)  loss_ce_0: 7.4747 (9.8176)  loss_bbox_0: 3.1635 (4.0178)  loss_giou_0: 1.7210 (1.7879)  loss_obj_ll_0: 147.1202 (187.1220)  loss_ce_1: 5.0849 (7.1025)  loss_bbox_1: 3.1251 (3.9915)  loss_giou_1: 1.7229 (1.7861)  loss_obj_ll_1: 112.0402 (170.6150)  loss_ce_2: 4.6764 (6.7323)  loss_bbox_2: 3.0913 (3.9769)  loss_giou_2: 1.7074 (1.7837)  loss_obj_ll_2: 119.9703 (170.8764)  loss_ce_3: 5.4298 (7.6729)  loss_bbox_3: 3.1266 (3.9854)  loss_giou_3: 1.7230 (1.7880)  loss_obj_ll_3: 129.2286 (180.3076)  loss_ce_4: 6.0058 (7.6477)  loss_bbox_4: 3.1918 (4.0234)  loss_giou_4: 1.7262 (1.7966)  loss_obj_ll_4: 113.7220 (174.3220)  loss_ce_unscaled: 2.5967 (3.6779)  class_error_unscaled: 100.0000 (98.6070)  loss_bbox_unscaled: 0.6512 (0.8085)  loss_giou_unscaled: 0.8673 (0.9005)  cardinality_error_unscaled: 12.0000 (25.5645)  loss_obj_ll_unscaled: 114.9613 (172.7754)  loss_ce_0_unscaled: 3.7373 (4.9088)  loss_bbox_0_unscaled: 0.6327 (0.8036)  loss_giou_0_unscaled: 0.8605 (0.8940)  cardinality_error_0_unscaled: 44.5000 (61.1452)  loss_obj_ll_0_unscaled: 147.1202 (187.1220)  loss_ce_1_unscaled: 2.5425 (3.5513)  loss_bbox_1_unscaled: 0.6250 (0.7983)  loss_giou_1_unscaled: 0.8614 (0.8931)  cardinality_error_1_unscaled: 4.5000 (29.2258)  loss_obj_ll_1_unscaled: 112.0402 (170.6150)  loss_ce_2_unscaled: 2.3382 (3.3661)  loss_bbox_2_unscaled: 0.6183 (0.7954)  loss_giou_2_unscaled: 0.8537 (0.8919)  cardinality_error_2_unscaled: 5.0000 (15.6935)  loss_obj_ll_2_unscaled: 119.9703 (170.8764)  loss_ce_3_unscaled: 2.7149 (3.8364)  loss_bbox_3_unscaled: 0.6253 (0.7971)  loss_giou_3_unscaled: 0.8615 (0.8940)  cardinality_error_3_unscaled: 6.5000 (31.4355)  loss_obj_ll_3_unscaled: 129.2286 (180.3076)  loss_ce_4_unscaled: 3.0029 (3.8238)  loss_bbox_4_unscaled: 0.6384 (0.8047)  loss_giou_4_unscaled: 0.8631 (0.8983)  cardinality_error_4_unscaled: 9.0000 (26.9516)  loss_obj_ll_4_unscaled: 113.7220 (174.3220)  time: 0.1137  data: 0.0000  max mem: 5861\n",
      "Epoch: [0]  [  40/8275]  eta: 0:19:04  lr: 0.000200  class_error: 100.00  grad_norm: 1025.04  loss: 681.0136 (1073.1170)  loss_ce: 4.3161 (6.4710)  loss_bbox: 2.8127 (3.6438)  loss_giou: 1.6095 (1.7338)  loss_obj_ll: 98.2041 (161.9253)  loss_ce_0: 6.0184 (8.7785)  loss_bbox_0: 2.7859 (3.6135)  loss_giou_0: 1.6050 (1.7135)  loss_obj_ll_0: 127.4449 (177.6621)  loss_ce_1: 4.4650 (6.3228)  loss_bbox_1: 2.7418 (3.6018)  loss_giou_1: 1.6367 (1.7115)  loss_obj_ll_1: 97.3663 (162.5919)  loss_ce_2: 4.4525 (5.9944)  loss_bbox_2: 2.7480 (3.5919)  loss_giou_2: 1.6284 (1.7083)  loss_obj_ll_2: 94.9731 (163.8735)  loss_ce_3: 5.0740 (6.8328)  loss_bbox_3: 2.7508 (3.5967)  loss_giou_3: 1.6119 (1.7127)  loss_obj_ll_3: 96.3734 (171.4712)  loss_ce_4: 4.9284 (6.8031)  loss_bbox_4: 2.8716 (3.6273)  loss_giou_4: 1.5765 (1.7238)  loss_obj_ll_4: 100.4679 (162.4116)  loss_ce_unscaled: 2.1580 (3.2355)  class_error_unscaled: 100.0000 (98.9468)  loss_bbox_unscaled: 0.5625 (0.7288)  loss_giou_unscaled: 0.8048 (0.8669)  cardinality_error_unscaled: 8.0000 (21.8293)  loss_obj_ll_unscaled: 98.2041 (161.9253)  loss_ce_0_unscaled: 3.0092 (4.3893)  loss_bbox_0_unscaled: 0.5572 (0.7227)  loss_giou_0_unscaled: 0.8025 (0.8567)  cardinality_error_0_unscaled: 12.5000 (49.2195)  loss_obj_ll_0_unscaled: 127.4449 (177.6621)  loss_ce_1_unscaled: 2.2325 (3.1614)  loss_bbox_1_unscaled: 0.5484 (0.7204)  loss_giou_1_unscaled: 0.8184 (0.8558)  cardinality_error_1_unscaled: 8.5000 (24.6829)  loss_obj_ll_1_unscaled: 97.3663 (162.5919)  loss_ce_2_unscaled: 2.2262 (2.9972)  loss_bbox_2_unscaled: 0.5496 (0.7184)  loss_giou_2_unscaled: 0.8142 (0.8542)  cardinality_error_2_unscaled: 9.0000 (14.5244)  loss_obj_ll_2_unscaled: 94.9731 (163.8735)  loss_ce_3_unscaled: 2.5370 (3.4164)  loss_bbox_3_unscaled: 0.5502 (0.7193)  loss_giou_3_unscaled: 0.8060 (0.8564)  cardinality_error_3_unscaled: 9.5000 (26.4268)  loss_obj_ll_3_unscaled: 96.3734 (171.4712)  loss_ce_4_unscaled: 2.4642 (3.4015)  loss_bbox_4_unscaled: 0.5743 (0.7255)  loss_giou_4_unscaled: 0.7882 (0.8619)  cardinality_error_4_unscaled: 8.5000 (23.0122)  loss_obj_ll_4_unscaled: 100.4679 (162.4116)  time: 0.1191  data: 0.0000  max mem: 9404\n",
      "Epoch: [0]  [  50/8275]  eta: 0:18:32  lr: 0.000200  class_error: 100.00  grad_norm: 5304.54  loss: 594.0096 (976.2701)  loss_ce: 3.7285 (6.0218)  loss_bbox: 2.3235 (3.4477)  loss_giou: 1.4179 (1.6531)  loss_obj_ll: 83.2088 (145.0788)  loss_ce_0: 5.0170 (8.2108)  loss_bbox_0: 2.2338 (3.4101)  loss_giou_0: 1.3295 (1.6295)  loss_obj_ll_0: 111.7004 (164.2764)  loss_ce_1: 3.7654 (5.8796)  loss_bbox_1: 2.2517 (3.4084)  loss_giou_1: 1.3644 (1.6294)  loss_obj_ll_1: 86.7134 (147.2781)  loss_ce_2: 3.4535 (5.6348)  loss_bbox_2: 2.3095 (3.3972)  loss_giou_2: 1.3578 (1.6259)  loss_obj_ll_2: 90.5951 (148.7414)  loss_ce_3: 3.9107 (6.3782)  loss_bbox_3: 2.2581 (3.3987)  loss_giou_3: 1.3811 (1.6315)  loss_obj_ll_3: 85.6202 (156.1413)  loss_ce_4: 4.0263 (6.3492)  loss_bbox_4: 2.3008 (3.4308)  loss_giou_4: 1.3626 (1.6431)  loss_obj_ll_4: 80.0643 (145.9744)  loss_ce_unscaled: 1.8642 (3.0109)  class_error_unscaled: 100.0000 (99.1533)  loss_bbox_unscaled: 0.4647 (0.6895)  loss_giou_unscaled: 0.7089 (0.8265)  cardinality_error_unscaled: 8.0000 (19.5686)  loss_obj_ll_unscaled: 83.2088 (145.0788)  loss_ce_0_unscaled: 2.5085 (4.1054)  loss_bbox_0_unscaled: 0.4468 (0.6820)  loss_giou_0_unscaled: 0.6647 (0.8148)  cardinality_error_0_unscaled: 10.0000 (41.9706)  loss_obj_ll_0_unscaled: 111.7004 (164.2764)  loss_ce_1_unscaled: 1.8827 (2.9398)  loss_bbox_1_unscaled: 0.4503 (0.6817)  loss_giou_1_unscaled: 0.6822 (0.8147)  cardinality_error_1_unscaled: 8.5000 (21.8529)  loss_obj_ll_1_unscaled: 86.7134 (147.2781)  loss_ce_2_unscaled: 1.7268 (2.8174)  loss_bbox_2_unscaled: 0.4619 (0.6794)  loss_giou_2_unscaled: 0.6789 (0.8130)  cardinality_error_2_unscaled: 8.5000 (13.8431)  loss_obj_ll_2_unscaled: 90.5951 (148.7414)  loss_ce_3_unscaled: 1.9553 (3.1891)  loss_bbox_3_unscaled: 0.4516 (0.6797)  loss_giou_3_unscaled: 0.6906 (0.8157)  cardinality_error_3_unscaled: 8.5000 (23.4510)  loss_obj_ll_3_unscaled: 85.6202 (156.1413)  loss_ce_4_unscaled: 2.0131 (3.1746)  loss_bbox_4_unscaled: 0.4602 (0.6862)  loss_giou_4_unscaled: 0.6813 (0.8216)  cardinality_error_4_unscaled: 8.0000 (20.6275)  loss_obj_ll_4_unscaled: 80.0643 (145.9744)  time: 0.1247  data: 0.0000  max mem: 9404\n",
      "Epoch: [0]  [  60/8275]  eta: 0:18:24  lr: 0.000200  class_error: 100.00  grad_norm: 2905.44  loss: 618.6268 (1050.0039)  loss_ce: 3.9893 (6.2681)  loss_bbox: 2.4595 (3.3147)  loss_giou: 1.3175 (1.6105)  loss_obj_ll: 84.4802 (151.8588)  loss_ce_0: 4.9705 (8.1163)  loss_bbox_0: 2.4795 (3.2839)  loss_giou_0: 1.2891 (1.5936)  loss_obj_ll_0: 110.0049 (174.0086)  loss_ce_1: 3.5658 (6.0075)  loss_bbox_1: 2.5325 (3.2830)  loss_giou_1: 1.2699 (1.5935)  loss_obj_ll_1: 86.7134 (159.5018)  loss_ce_2: 3.4569 (5.8743)  loss_bbox_2: 2.4443 (3.2727)  loss_giou_2: 1.2737 (1.5923)  loss_obj_ll_2: 91.3756 (164.0534)  loss_ce_3: 3.9107 (6.5526)  loss_bbox_3: 2.4431 (3.2651)  loss_giou_3: 1.3131 (1.6008)  loss_obj_ll_3: 97.8690 (172.7679)  loss_ce_4: 4.0066 (6.5265)  loss_bbox_4: 2.4664 (3.2991)  loss_giou_4: 1.3250 (1.6097)  loss_obj_ll_4: 80.9799 (159.1491)  loss_ce_unscaled: 1.9946 (3.1340)  class_error_unscaled: 100.0000 (99.1828)  loss_bbox_unscaled: 0.4919 (0.6629)  loss_giou_unscaled: 0.6587 (0.8053)  cardinality_error_unscaled: 10.5000 (19.6230)  loss_obj_ll_unscaled: 84.4802 (151.8588)  loss_ce_0_unscaled: 2.4852 (4.0581)  loss_bbox_0_unscaled: 0.4959 (0.6568)  loss_giou_0_unscaled: 0.6446 (0.7968)  cardinality_error_0_unscaled: 11.5000 (38.4508)  loss_obj_ll_0_unscaled: 110.0049 (174.0086)  loss_ce_1_unscaled: 1.7829 (3.0037)  loss_bbox_1_unscaled: 0.5065 (0.6566)  loss_giou_1_unscaled: 0.6350 (0.7967)  cardinality_error_1_unscaled: 10.0000 (21.4098)  loss_obj_ll_1_unscaled: 86.7134 (159.5018)  loss_ce_2_unscaled: 1.7285 (2.9372)  loss_bbox_2_unscaled: 0.4889 (0.6545)  loss_giou_2_unscaled: 0.6369 (0.7961)  cardinality_error_2_unscaled: 11.0000 (14.8033)  loss_obj_ll_2_unscaled: 91.3756 (164.0534)  loss_ce_3_unscaled: 1.9553 (3.2763)  loss_bbox_3_unscaled: 0.4886 (0.6530)  loss_giou_3_unscaled: 0.6565 (0.8004)  cardinality_error_3_unscaled: 11.0000 (22.8852)  loss_obj_ll_3_unscaled: 97.8690 (172.7679)  loss_ce_4_unscaled: 2.0033 (3.2632)  loss_bbox_4_unscaled: 0.4933 (0.6598)  loss_giou_4_unscaled: 0.6625 (0.8049)  cardinality_error_4_unscaled: 11.0000 (20.5246)  loss_obj_ll_4_unscaled: 80.9799 (159.1491)  time: 0.1253  data: 0.0000  max mem: 9404\n",
      "Epoch: [0]  [  70/8275]  eta: 0:17:55  lr: 0.000200  class_error: 100.00  grad_norm: 37162.25  loss: 1115.5034 (1108.0114)  loss_ce: 4.8973 (6.1898)  loss_bbox: 2.4998 (3.1999)  loss_giou: 1.3162 (1.5624)  loss_obj_ll: 170.6590 (159.6905)  loss_ce_0: 5.1180 (7.7767)  loss_bbox_0: 2.4819 (3.1714)  loss_giou_0: 1.2841 (1.5572)  loss_obj_ll_0: 161.6438 (189.0444)  loss_ce_1: 3.8716 (5.7903)  loss_bbox_1: 2.5410 (3.1821)  loss_giou_1: 1.2368 (1.5496)  loss_obj_ll_1: 184.2445 (173.6761)  loss_ce_2: 4.3345 (5.7471)  loss_bbox_2: 2.5037 (3.1668)  loss_giou_2: 1.2250 (1.5516)  loss_obj_ll_2: 187.6391 (173.6242)  loss_ce_3: 4.5856 (6.3959)  loss_bbox_3: 2.4459 (3.1591)  loss_giou_3: 1.1885 (1.5603)  loss_obj_ll_3: 187.3703 (179.8251)  loss_ce_4: 4.8109 (6.4099)  loss_bbox_4: 2.4664 (3.1869)  loss_giou_4: 1.2125 (1.5601)  loss_obj_ll_4: 162.4947 (165.4342)  loss_ce_unscaled: 2.4486 (3.0949)  class_error_unscaled: 100.0000 (99.2979)  loss_bbox_unscaled: 0.5000 (0.6400)  loss_giou_unscaled: 0.6581 (0.7812)  cardinality_error_unscaled: 11.5000 (18.8873)  loss_obj_ll_unscaled: 170.6590 (159.6905)  loss_ce_0_unscaled: 2.5590 (3.8884)  loss_bbox_0_unscaled: 0.4964 (0.6343)  loss_giou_0_unscaled: 0.6421 (0.7786)  cardinality_error_0_unscaled: 11.0000 (34.9155)  loss_obj_ll_0_unscaled: 161.6438 (189.0444)  loss_ce_1_unscaled: 1.9358 (2.8951)  loss_bbox_1_unscaled: 0.5082 (0.6364)  loss_giou_1_unscaled: 0.6184 (0.7748)  cardinality_error_1_unscaled: 7.5000 (19.9225)  loss_obj_ll_1_unscaled: 184.2445 (173.6761)  loss_ce_2_unscaled: 2.1673 (2.8736)  loss_bbox_2_unscaled: 0.5007 (0.6334)  loss_giou_2_unscaled: 0.6125 (0.7758)  cardinality_error_2_unscaled: 8.5000 (14.5211)  loss_obj_ll_2_unscaled: 187.6391 (173.6242)  loss_ce_3_unscaled: 2.2928 (3.1979)  loss_bbox_3_unscaled: 0.4892 (0.6318)  loss_giou_3_unscaled: 0.5942 (0.7802)  cardinality_error_3_unscaled: 11.0000 (21.6197)  loss_obj_ll_3_unscaled: 187.3703 (179.8251)  loss_ce_4_unscaled: 2.4054 (3.2050)  loss_bbox_4_unscaled: 0.4933 (0.6374)  loss_giou_4_unscaled: 0.6062 (0.7801)  cardinality_error_4_unscaled: 11.0000 (19.6268)  loss_obj_ll_4_unscaled: 162.4947 (165.4342)  time: 0.1204  data: 0.0000  max mem: 9404\n",
      "Epoch: [0]  [  80/8275]  eta: 0:17:42  lr: 0.000200  class_error: 100.00  grad_norm: 18129.14  loss: 1891.4287 (1276.7858)  loss_ce: 3.6206 (5.8491)  loss_bbox: 2.7793 (3.2071)  loss_giou: 1.4357 (1.5794)  loss_obj_ll: 268.1355 (188.5532)  loss_ce_0: 4.2032 (7.3240)  loss_bbox_0: 2.8865 (3.1740)  loss_giou_0: 1.4312 (1.5778)  loss_obj_ll_0: 361.8271 (220.9351)  loss_ce_1: 3.1380 (5.4669)  loss_bbox_1: 2.9535 (3.2212)  loss_giou_1: 1.4200 (1.5602)  loss_obj_ll_1: 382.2558 (193.9401)  loss_ce_2: 3.4355 (5.4456)  loss_bbox_2: 2.8911 (3.2093)  loss_giou_2: 1.4243 (1.5628)  loss_obj_ll_2: 255.0413 (193.4801)  loss_ce_3: 3.6757 (6.0296)  loss_bbox_3: 2.5329 (3.1883)  loss_giou_3: 1.4355 (1.5861)  loss_obj_ll_3: 267.6417 (215.2305)  loss_ce_4: 3.5237 (6.0378)  loss_bbox_4: 2.7238 (3.2029)  loss_giou_4: 1.3797 (1.5821)  loss_obj_ll_4: 279.4182 (199.8427)  loss_ce_unscaled: 1.8103 (2.9245)  class_error_unscaled: 100.0000 (99.3846)  loss_bbox_unscaled: 0.5559 (0.6414)  loss_giou_unscaled: 0.7178 (0.7897)  cardinality_error_unscaled: 7.5000 (17.3519)  loss_obj_ll_unscaled: 268.1355 (188.5532)  loss_ce_0_unscaled: 2.1016 (3.6620)  loss_bbox_0_unscaled: 0.5773 (0.6348)  loss_giou_0_unscaled: 0.7156 (0.7889)  cardinality_error_0_unscaled: 9.0000 (31.7963)  loss_obj_ll_0_unscaled: 361.8271 (220.9351)  loss_ce_1_unscaled: 1.5690 (2.7335)  loss_bbox_1_unscaled: 0.5907 (0.6442)  loss_giou_1_unscaled: 0.7100 (0.7801)  cardinality_error_1_unscaled: 5.0000 (18.1605)  loss_obj_ll_1_unscaled: 382.2558 (193.9401)  loss_ce_2_unscaled: 1.7177 (2.7228)  loss_bbox_2_unscaled: 0.5782 (0.6419)  loss_giou_2_unscaled: 0.7121 (0.7814)  cardinality_error_2_unscaled: 7.0000 (13.6049)  loss_obj_ll_2_unscaled: 255.0413 (193.4801)  loss_ce_3_unscaled: 1.8379 (3.0148)  loss_bbox_3_unscaled: 0.5066 (0.6377)  loss_giou_3_unscaled: 0.7178 (0.7931)  cardinality_error_3_unscaled: 7.5000 (19.8827)  loss_obj_ll_3_unscaled: 267.6417 (215.2305)  loss_ce_4_unscaled: 1.7619 (3.0189)  loss_bbox_4_unscaled: 0.5448 (0.6406)  loss_giou_4_unscaled: 0.6898 (0.7911)  cardinality_error_4_unscaled: 8.5000 (18.0062)  loss_obj_ll_4_unscaled: 279.4182 (199.8427)  time: 0.1149  data: 0.0000  max mem: 9404\n",
      "Epoch: [0]  [  90/8275]  eta: 0:17:34  lr: 0.000200  class_error: 100.00  grad_norm: 19415.76  loss: 2039.7172 (1310.0344)  loss_ce: 3.2734 (5.7627)  loss_bbox: 2.7064 (3.1385)  loss_giou: 1.6350 (1.5700)  loss_obj_ll: 256.3827 (193.2254)  loss_ce_0: 4.2032 (7.1900)  loss_bbox_0: 2.6829 (3.0947)  loss_giou_0: 1.8148 (1.5784)  loss_obj_ll_0: 357.5847 (227.3729)  loss_ce_1: 3.1147 (5.4287)  loss_bbox_1: 2.7549 (3.1463)  loss_giou_1: 1.6517 (1.5571)  loss_obj_ll_1: 331.9853 (202.4814)  loss_ce_2: 3.1728 (5.4390)  loss_bbox_2: 2.8595 (3.1401)  loss_giou_2: 1.6023 (1.5570)  loss_obj_ll_2: 290.3164 (200.9618)  loss_ce_3: 3.3617 (5.9362)  loss_bbox_3: 2.7895 (3.1225)  loss_giou_3: 1.8240 (1.5748)  loss_obj_ll_3: 296.7711 (219.3940)  loss_ce_4: 3.1642 (5.9106)  loss_bbox_4: 2.9491 (3.1429)  loss_giou_4: 1.6625 (1.5672)  loss_obj_ll_4: 237.9867 (202.7422)  loss_ce_unscaled: 1.6367 (2.8814)  class_error_unscaled: 100.0000 (99.4522)  loss_bbox_unscaled: 0.5413 (0.6277)  loss_giou_unscaled: 0.8175 (0.7850)  cardinality_error_unscaled: 6.5000 (17.6593)  loss_obj_ll_unscaled: 256.3827 (193.2254)  loss_ce_0_unscaled: 2.1016 (3.5950)  loss_bbox_0_unscaled: 0.5366 (0.6189)  loss_giou_0_unscaled: 0.9074 (0.7892)  cardinality_error_0_unscaled: 10.5000 (31.1648)  loss_obj_ll_0_unscaled: 357.5847 (227.3729)  loss_ce_1_unscaled: 1.5574 (2.7144)  loss_bbox_1_unscaled: 0.5510 (0.6293)  loss_giou_1_unscaled: 0.8258 (0.7786)  cardinality_error_1_unscaled: 5.0000 (18.4451)  loss_obj_ll_1_unscaled: 331.9853 (202.4814)  loss_ce_2_unscaled: 1.5864 (2.7195)  loss_bbox_2_unscaled: 0.5719 (0.6280)  loss_giou_2_unscaled: 0.8012 (0.7785)  cardinality_error_2_unscaled: 7.5000 (14.5604)  loss_obj_ll_2_unscaled: 290.3164 (200.9618)  loss_ce_3_unscaled: 1.6809 (2.9681)  loss_bbox_3_unscaled: 0.5579 (0.6245)  loss_giou_3_unscaled: 0.9120 (0.7874)  cardinality_error_3_unscaled: 7.5000 (20.0000)  loss_obj_ll_3_unscaled: 296.7711 (219.3940)  loss_ce_4_unscaled: 1.5821 (2.9553)  loss_bbox_4_unscaled: 0.5898 (0.6286)  loss_giou_4_unscaled: 0.8312 (0.7836)  cardinality_error_4_unscaled: 7.0000 (18.0714)  loss_obj_ll_4_unscaled: 237.9867 (202.7422)  time: 0.1208  data: 0.0000  max mem: 9404\n",
      "Averaged stats: lr: 0.000200  class_error: 100.00  grad_norm: 24525.69  loss: 1570.1892 (1342.2846)  loss_ce: 3.4597 (5.5868)  loss_bbox: 2.6855 (3.0907)  loss_giou: 1.3691 (1.5631)  loss_obj_ll: 221.5846 (196.8329)  loss_ce_0: 4.5312 (6.9575)  loss_bbox_0: 2.6364 (3.0497)  loss_giou_0: 1.6231 (1.5812)  loss_obj_ll_0: 274.8632 (232.6544)  loss_ce_1: 3.6817 (5.2668)  loss_bbox_1: 2.6598 (3.1054)  loss_giou_1: 1.4625 (1.5576)  loss_obj_ll_1: 250.9821 (209.4251)  loss_ce_2: 3.8665 (5.2934)  loss_bbox_2: 2.7979 (3.0949)  loss_giou_2: 1.5288 (1.5609)  loss_obj_ll_2: 265.0730 (207.9998)  loss_ce_3: 3.5049 (5.7401)  loss_bbox_3: 2.6878 (3.0783)  loss_giou_3: 1.3056 (1.5743)  loss_obj_ll_3: 238.5976 (224.3632)  loss_ce_4: 3.1177 (5.7133)  loss_bbox_4: 2.6396 (3.0941)  loss_giou_4: 1.3361 (1.5661)  loss_obj_ll_4: 227.2391 (208.5348)  loss_ce_unscaled: 1.7298 (2.7934)  class_error_unscaled: 100.0000 (99.5065)  loss_bbox_unscaled: 0.5371 (0.6181)  loss_giou_unscaled: 0.6845 (0.7815)  cardinality_error_unscaled: 13.0000 (17.0842)  loss_obj_ll_unscaled: 221.5846 (196.8329)  loss_ce_0_unscaled: 2.2656 (3.4788)  loss_bbox_0_unscaled: 0.5273 (0.6099)  loss_giou_0_unscaled: 0.8116 (0.7906)  cardinality_error_0_unscaled: 16.5000 (29.9158)  loss_obj_ll_0_unscaled: 274.8632 (232.6544)  loss_ce_1_unscaled: 1.8409 (2.6334)  loss_bbox_1_unscaled: 0.5320 (0.6211)  loss_giou_1_unscaled: 0.7312 (0.7788)  cardinality_error_1_unscaled: 11.5000 (17.7030)  loss_obj_ll_1_unscaled: 250.9821 (209.4251)  loss_ce_2_unscaled: 1.9333 (2.6467)  loss_bbox_2_unscaled: 0.5596 (0.6190)  loss_giou_2_unscaled: 0.7644 (0.7804)  cardinality_error_2_unscaled: 12.0000 (14.3416)  loss_obj_ll_2_unscaled: 265.0730 (207.9998)  loss_ce_3_unscaled: 1.7524 (2.8701)  loss_bbox_3_unscaled: 0.5376 (0.6157)  loss_giou_3_unscaled: 0.6528 (0.7871)  cardinality_error_3_unscaled: 10.5000 (19.1485)  loss_obj_ll_3_unscaled: 238.5976 (224.3632)  loss_ce_4_unscaled: 1.5588 (2.8567)  loss_bbox_4_unscaled: 0.5279 (0.6188)  loss_giou_4_unscaled: 0.6680 (0.7831)  cardinality_error_4_unscaled: 9.5000 (17.3416)  loss_obj_ll_4_unscaled: 227.2391 (208.5348)\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import json\n",
    "import math\n",
    "import os\n",
    "import sys\n",
    "\n",
    "from typing import Iterable\n",
    "from datasets.data_prefetcher import data_prefetcher\n",
    "from copy import deepcopy\n",
    "\n",
    "\n",
    "wandb = None\n",
    "args.distributed = False\n",
    "\n",
    "def train_one_epoch(model: torch.nn.Module, criterion: torch.nn.Module,\n",
    "                    data_loader: Iterable, optimizer: torch.optim.Optimizer,\n",
    "                    device: torch.device, epoch: int, nc_epoch: int, max_norm: float = 0, wandb: object = None):\n",
    "    model.train()\n",
    "    criterion.train()\n",
    "    metric_logger = utils.MetricLogger(delimiter=\"  \")\n",
    "    metric_logger.add_meter('lr', utils.SmoothedValue(window_size=1, fmt='{value:.6f}'))\n",
    "    metric_logger.add_meter('class_error', utils.SmoothedValue(window_size=1, fmt='{value:.2f}'))\n",
    "    metric_logger.add_meter('grad_norm', utils.SmoothedValue(window_size=1, fmt='{value:.2f}'))\n",
    "    header = 'Epoch: [{}]'.format(epoch)\n",
    "    print_freq = 10\n",
    "    prefetcher = data_prefetcher(data_loader, device, prefetch=True)\n",
    "    samples, targets = prefetcher.next()\n",
    "\n",
    "    for _idx, _ in enumerate(metric_logger.log_every(range(len(data_loader)), print_freq, header)):\n",
    "        outputs = model(samples)\n",
    "        loss_dict = criterion(outputs, targets) \n",
    "        weight_dict = deepcopy(criterion.weight_dict)\n",
    "        \n",
    "        ## condition for starting nc loss computation after certain epoch so that the F_cls branch has the time\n",
    "        ## to learn the within classes seperation.\n",
    "        if epoch < nc_epoch: \n",
    "            for k,v in weight_dict.items():\n",
    "                if 'NC' in k:\n",
    "                    weight_dict[k] = 0\n",
    "         \n",
    "        losses = sum(loss_dict[k] * weight_dict[k] for k in loss_dict.keys() if k in weight_dict)\n",
    "        # reduce losses over all GPUs for logging purposes\n",
    "\n",
    "        loss_dict_reduced = utils.reduce_dict(loss_dict)\n",
    "        ## Just printing NOt affectin gin loss function\n",
    "        loss_dict_reduced_unscaled = {f'{k}_unscaled': v\n",
    "                                      for k, v in loss_dict_reduced.items()}\n",
    "        loss_dict_reduced_scaled = {k: v * weight_dict[k]\n",
    "                                    for k, v in loss_dict_reduced.items() if k in weight_dict}\n",
    "        losses_reduced_scaled = sum(loss_dict_reduced_scaled.values())\n",
    " \n",
    "        loss_value = losses_reduced_scaled.item()\n",
    " \n",
    "        if not math.isfinite(loss_value):\n",
    "            print(\"Loss is {}, stopping training\".format(loss_value))\n",
    "            print(loss_dict_reduced)\n",
    "            sys.exit(1)\n",
    " \n",
    "        optimizer.zero_grad()\n",
    "        losses.backward()\n",
    "        if max_norm > 0:\n",
    "            grad_total_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm)\n",
    "        else:\n",
    "            grad_total_norm = utils.get_total_grad_norm(model.parameters(), max_norm)\n",
    "        optimizer.step()\n",
    "        \n",
    "        if wandb is not None:\n",
    "            wandb.log({\"total_loss\":loss_value})\n",
    "            wandb.log(loss_dict_reduced_scaled)\n",
    "            wandb.log(loss_dict_reduced_unscaled)\n",
    " \n",
    "        metric_logger.update(loss=loss_value, **loss_dict_reduced_scaled, **loss_dict_reduced_unscaled)\n",
    "        metric_logger.update(class_error=loss_dict_reduced['class_error'])\n",
    "        metric_logger.update(lr=optimizer.param_groups[0][\"lr\"])\n",
    "        metric_logger.update(grad_norm=grad_total_norm)\n",
    "        \n",
    "        samples, targets = prefetcher.next()\n",
    "        if _idx == 100 :\n",
    "            break\n",
    "    # gather the stats from all processes\n",
    "    metric_logger.synchronize_between_processes()\n",
    "    print(\"Averaged stats:\", metric_logger)\n",
    "    return {k: meter.global_avg for k, meter in metric_logger.meters.items()}\n",
    "\n",
    "print(f'Start training from epoch {args.start_epoch} to {args.epochs}')\n",
    "start_time = time.time()\n",
    "for epoch in range(args.start_epoch, args.epochs):\n",
    "    if args.distributed:\n",
    "        sampler_train.set_epoch(epoch)\n",
    "        \n",
    "    train_stats = train_one_epoch(\n",
    "        model, criterion, data_loader_train, optimizer, device, epoch, args.nc_epoch, args.clip_max_norm, wandb)\n",
    "        \n",
    "    lr_scheduler.step()\n",
    "    if args.output_dir:\n",
    "        checkpoint_paths = [output_dir / 'checkpoint.pth']\n",
    "        # extra checkpoint before LR drop and every 5 epochs\n",
    "        if (epoch + 1) % args.lr_drop == 0 or (epoch % args.eval_every == 0 or epoch == 0 or epoch == 1 or (args.epochs-epoch)<1):\n",
    "            test_stats, coco_evaluator = evaluate(\n",
    "                model, criterion, postprocessors, data_loader_val, base_ds, device, args.output_dir, args)\n",
    "            checkpoint_paths.append(output_dir / f'checkpoint{epoch:04}.pth')\n",
    "            if wandb is not None:\n",
    "                test_stats[\"metrics\"]['epoch']=epoch\n",
    "                wandb.log({str(key): val for key, val in test_stats[\"metrics\"].items()})\n",
    "        elif epoch > args.epochs-6:\n",
    "            checkpoint_paths.append(output_dir / f'checkpoint{epoch:04}.pth')\n",
    "        else:\n",
    "                test_stats = {}\n",
    "                \n",
    "        for checkpoint_path in checkpoint_paths:\n",
    "            utils.save_on_master({\n",
    "                'model': model_without_ddp.state_dict(),\n",
    "                'optimizer': optimizer.state_dict(),\n",
    "                'lr_scheduler': lr_scheduler.state_dict(),\n",
    "                'epoch': epoch,\n",
    "                'args': args,\n",
    "            }, checkpoint_path)\n",
    "    try:        \n",
    "        log_stats = {**{f'train_{k}': v for k, v in train_stats.items()},\n",
    "                        **{f'test_{k}': v for k, v in test_stats.items()},\n",
    "                        'epoch': epoch,\n",
    "                        'n_parameters': n_parameters}\n",
    "    except:\n",
    "        log_stats = {'None' : 'None'}\n",
    "    \n",
    "    if args.output_dir and utils.is_main_process():\n",
    "        with (output_dir / \"log.txt\").open(\"a\") as f:\n",
    "            f.write(json.dumps(log_stats) + \"\\n\")\n",
    "        if args.dataset in ['owod', 'owdetr'] and epoch % args.eval_every == 0 and epoch > 0:\n",
    "            # for evaluation logs\n",
    "            if coco_evaluator is not None:\n",
    "                (output_dir / 'eval').mkdir(exist_ok=True)\n",
    "                if \"bbox\" in coco_evaluator.coco_eval:\n",
    "                    filenames = ['latest.pth']\n",
    "                    if epoch % 50 == 0:\n",
    "                        filenames.append(f'{epoch:03}.pth')\n",
    "                    for name in filenames:\n",
    "                        torch.save(coco_evaluator.coco_eval[\"bbox\"].eval,\n",
    "                                output_dir / \"eval\" / name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ExempReplay]  [   0/8275]  eta: 0:05:36  loss: 2.0000 (2.0000)  time: 0.0406  data: 0.0000  max mem: 9404\n",
      "[ExempReplay]  [  10/8275]  eta: 0:06:19  loss: 12.0000 (12.0000)  time: 0.0459  data: 0.0000  max mem: 10028\n",
      "[ExempReplay]  [  20/8275]  eta: 0:06:31  loss: 22.0000 (22.0000)  time: 0.0477  data: 0.0000  max mem: 10347\n",
      "[ExempReplay]  [  30/8275]  eta: 0:06:17  loss: 42.0000 (32.0000)  time: 0.0457  data: 0.0000  max mem: 10499\n",
      "[ExempReplay]  [  40/8275]  eta: 0:06:13  loss: 62.0000 (42.0000)  time: 0.0432  data: 0.0000  max mem: 10637\n",
      "[ExempReplay]  [  50/8275]  eta: 0:06:11  loss: 82.0000 (52.0000)  time: 0.0443  data: 0.0000  max mem: 10637\n",
      "[ExempReplay]  [  60/8275]  eta: 0:06:11  loss: 102.0000 (62.0000)  time: 0.0450  data: 0.0000  max mem: 10637\n",
      "[ExempReplay]  [  70/8275]  eta: 0:06:15  loss: 122.0000 (72.0000)  time: 0.0473  data: 0.0000  max mem: 11084\n",
      "[ExempReplay]  [  80/8275]  eta: 0:06:18  loss: 142.0000 (82.0000)  time: 0.0489  data: 0.0000  max mem: 11084\n",
      "[ExempReplay]  [  90/8275]  eta: 0:06:19  loss: 162.0000 (92.0000)  time: 0.0484  data: 0.0000  max mem: 11324\n",
      "found a total of 202 images\n",
      "found a total of 202 images\n",
      "only found 34 imgs in class 0\n",
      "only found 21 imgs in class 1\n",
      "only found 21 imgs in class 2\n",
      "only found 12 imgs in class 3\n",
      "only found 19 imgs in class 4\n",
      "only found 14 imgs in class 5\n",
      "only found 31 imgs in class 6\n",
      "only found 15 imgs in class 7\n",
      "only found 31 imgs in class 8\n",
      "only found 10 imgs in class 9\n",
      "only found 9 imgs in class 10\n",
      "only found 30 imgs in class 11\n",
      "only found 21 imgs in class 12\n",
      "only found 15 imgs in class 13\n",
      "only found 9 imgs in class 15\n",
      "only found 28 imgs in class 16\n",
      "only found 18 imgs in class 17\n",
      "only found 13 imgs in class 18\n",
      "only found 13 imgs in class 19\n",
      "found 190 images in run\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "args.exemplar_replay_selection = True\n",
    "\n",
    "def get_exemplar_replay(model, exemplar_selection, device, data_loader):\n",
    "    metric_logger = utils.MetricLogger(delimiter=\"  \")\n",
    "    header = '[ExempReplay]'\n",
    "    print_freq = 10\n",
    "    prefetcher = data_prefetcher(data_loader, device, prefetch=True)\n",
    "    samples, targets = prefetcher.next()\n",
    "    image_sorted_scores_reduced={}\n",
    "    for _idx, _ in enumerate(metric_logger.log_every(range(len(data_loader)), print_freq, header)):\n",
    "        outputs = model(samples)\n",
    "        image_sorted_scores = exemplar_selection(samples, outputs, targets)\n",
    "        for i in utils.combine_dict(image_sorted_scores):\n",
    "            image_sorted_scores_reduced.update(i[0])\n",
    "            \n",
    "        metric_logger.update(loss=len(image_sorted_scores_reduced.keys()))\n",
    "        samples, targets = prefetcher.next()\n",
    "        \n",
    "        if _idx == 100 :\n",
    "            break\n",
    "        \n",
    "    print(f'found a total of {len(image_sorted_scores_reduced.keys())} images')\n",
    "    return image_sorted_scores_reduced\n",
    "\n",
    "\n",
    "def create_ft_dataset(args, image_sorted_scores):\n",
    "    print(f'found a total of {len(image_sorted_scores.keys())} images')\n",
    "    tmp_dir=args.data_root +'/ImageSets/'+args.dataset+\"/\"+args.exemplar_replay_dir+\"/\"\n",
    "    #tmp_dir=args.data_root +'/ImageSets/'+args.exemplar_replay_dir+\"/\"\n",
    "\n",
    "    class_sorted_scores={}\n",
    "    imgs_per_class={}\n",
    "    for i in range(args.PREV_INTRODUCED_CLS, args.CUR_INTRODUCED_CLS+args.PREV_INTRODUCED_CLS):\n",
    "        class_sorted_scores[str(i)]=[]\n",
    "        imgs_per_class[str(i)]=[]\n",
    "\n",
    "    for k,v in image_sorted_scores.items():\n",
    "        for j in range(len(v['labels'])):\n",
    "            class_sorted_scores[str(v['labels'][j])].append(v['scores'][j])\n",
    "\n",
    "\n",
    "    class_threshold={}\n",
    "    for i in range(args.PREV_INTRODUCED_CLS, args.CUR_INTRODUCED_CLS+args.PREV_INTRODUCED_CLS):\n",
    "        tmp=np.array(class_sorted_scores[str(i)])\n",
    "        tmp.sort()\n",
    "        tmp = torch.Tensor(tmp)\n",
    "        if len(tmp)>args.num_inst_per_class and not args.exemplar_replay_random:\n",
    "            max_val = tmp[-args.num_inst_per_class//2]\n",
    "            min_val = tmp[args.num_inst_per_class//2]\n",
    "        else:\n",
    "            if args.exemplar_replay_random:\n",
    "                print('using random exemplar selection')\n",
    "            else:\n",
    "                print(f'only found {len(tmp)} imgs in class {i}')\n",
    "            max_val = tmp.min()\n",
    "            min_val = tmp.max()\n",
    "            \n",
    "        class_threshold[str(i)]=(min_val, max_val)\n",
    "\n",
    "    save_imgs = []    \n",
    "    for k,v in image_sorted_scores.items():\n",
    "        for j in range(len(v['labels'])):\n",
    "            label = str(v['labels'][j])\n",
    "            if (v['scores'][j] <= class_threshold[label][0].numpy() or v['scores'][j] >= class_threshold[label][1].numpy()) and (len(imgs_per_class[label])<=args.num_inst_per_class+2):\n",
    "                save_imgs.append(k)\n",
    "                imgs_per_class[label].append(k)\n",
    "                        \n",
    "    print(f'found {len(np.unique(save_imgs))} images in run')\n",
    "    if len(args.exemplar_replay_prev_file)>0:\n",
    "        previous_ft = open(tmp_dir+args.exemplar_replay_prev_file,'r').read().splitlines()\n",
    "        save_imgs+=previous_ft\n",
    "        \n",
    "    save_imgs=np.unique(save_imgs)\n",
    "    np.random.shuffle(save_imgs)\n",
    "    if len(save_imgs)> args.exemplar_replay_max_length:\n",
    "        save_imgs=save_imgs[:args.exemplar_replay_max_length]\n",
    "    \n",
    "    os.makedirs(tmp_dir, exist_ok=True)\n",
    "    with open('replay.txt', 'w') as f: # 그냥 바로 root 폴더에 replay.txt 저장하도록 수정\n",
    "        for line in save_imgs:\n",
    "            f.write(line)\n",
    "            f.write('\\n')\n",
    "    return\n",
    "\n",
    "if args.exemplar_replay_selection:\n",
    "    image_sorted_scores = get_exemplar_replay(model,exemplar_selection, device, data_loader_train)\n",
    "    create_ft_dataset(args, image_sorted_scores)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
