{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PROB Demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 기존의 PROB 코드에서 사용하던 argparser 그대로 사용하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(lr=0.0002\n",
      "lr_backbone_names=['backbone.0']\n",
      "lr_backbone=2e-05\n",
      "lr_linear_proj_names=['reference_points'\n",
      "'sampling_offsets']\n",
      "lr_linear_proj_mult=0.1\n",
      "batch_size=2\n",
      "weight_decay=0.0001\n",
      "epochs=51\n",
      "lr_drop=35\n",
      "lr_drop_epochs=None\n",
      "clip_max_norm=0.1\n",
      "sgd=False\n",
      "with_box_refine=False\n",
      "two_stage=False\n",
      "masks=False\n",
      "backbone='dino_resnet50'\n",
      "frozen_weights=None\n",
      "dilation=False\n",
      "position_embedding='sine'\n",
      "position_embedding_scale=6.283185307179586\n",
      "num_feature_levels=4\n",
      "enc_layers=6\n",
      "dec_layers=6\n",
      "dim_feedforward=1024\n",
      "hidden_dim=256\n",
      "dropout=0.1\n",
      "nheads=8\n",
      "num_queries=100\n",
      "dec_n_points=4\n",
      "enc_n_points=4\n",
      "aux_loss=True\n",
      "set_cost_class=2\n",
      "set_cost_bbox=5\n",
      "set_cost_giou=2\n",
      "cls_loss_coef=2\n",
      "bbox_loss_coef=5\n",
      "giou_loss_coef=2\n",
      "focal_alpha=0.25\n",
      "coco_panoptic_path=None\n",
      "remove_difficult=False\n",
      "output_dir=''\n",
      "device='cuda'\n",
      "seed=42\n",
      "resume=''\n",
      "start_epoch=0\n",
      "eval=False\n",
      "viz=False\n",
      "eval_every=5\n",
      "num_workers=3\n",
      "cache_mode=False\n",
      "PREV_INTRODUCED_CLS=0\n",
      "CUR_INTRODUCED_CLS=20\n",
      "unmatched_boxes=False\n",
      "top_unk=5\n",
      "featdim=1024\n",
      "invalid_cls_logits=False\n",
      "NC_branch=False\n",
      "bbox_thresh=0.3\n",
      "pretrain=''\n",
      "nc_loss_coef=2\n",
      "train_set='owod_t1_5classes_train'\n",
      "test_set='owdetr_test'\n",
      "num_classes=81\n",
      "nc_epoch=0\n",
      "dataset='OWDETR'\n",
      "data_root='./data/OWOD'\n",
      "unk_conf_w=1.0\n",
      "model_type='prob'\n",
      "wandb_name=''\n",
      "wandb_project=''\n",
      "obj_loss_coef=1\n",
      "obj_temp=1\n",
      "freeze_prob_model=False\n",
      "num_inst_per_class=50\n",
      "exemplar_replay_selection=False\n",
      "exemplar_replay_max_length=10000000000.0\n",
      "exemplar_replay_dir=''\n",
      "exemplar_replay_prev_file=''\n",
      "exemplar_replay_cur_file=''\n",
      "exemplar_replay_random=False)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import argparse\n",
    "\n",
    "def get_args_parser():\n",
    "    parser = argparse.ArgumentParser('Defromable DETR Detector', add_help=False)\n",
    "\n",
    "    ################ Deformable DETR ################\n",
    "    parser.add_argument('--lr', default=2e-4, type=float)\n",
    "    parser.add_argument('--lr_backbone_names', default=[\"backbone.0\"], type=str, nargs='+')\n",
    "    parser.add_argument('--lr_backbone', default=2e-5, type=float)\n",
    "    parser.add_argument('--lr_linear_proj_names', default=['reference_points', 'sampling_offsets'], type=str, nargs='+')\n",
    "    parser.add_argument('--lr_linear_proj_mult', default=0.1, type=float)\n",
    "    parser.add_argument('--batch_size', default=2, type=int)\n",
    "    parser.add_argument('--weight_decay', default=1e-4, type=float)\n",
    "    parser.add_argument('--epochs', default=51, type=int)\n",
    "    parser.add_argument('--lr_drop', default=35, type=int)\n",
    "    parser.add_argument('--lr_drop_epochs', default=None, type=int, nargs='+')\n",
    "    parser.add_argument('--clip_max_norm', default=0.1, type=float,\n",
    "                        help='gradient clipping max norm')\n",
    "    parser.add_argument('--sgd', action='store_true')\n",
    "\n",
    "    # Variants of Deformable DETR\n",
    "    parser.add_argument('--with_box_refine', default=False, action='store_true')\n",
    "    parser.add_argument('--two_stage', default=False, action='store_true')\n",
    "    parser.add_argument('--masks', default=False, action='store_true', help=\"Train segmentation head if the flag is provided\")\n",
    "    parser.add_argument('--backbone', default='dino_resnet50', type=str, help=\"Name of the convolutional backbone to use\")\n",
    "\n",
    "    # Model parameters\n",
    "    parser.add_argument('--frozen_weights', type=str, default=None,\n",
    "                        help=\"Path to the pretrained model. If set, only the mask head will be trained\")\n",
    "    parser.add_argument('--dilation', action='store_true',\n",
    "                        help=\"If true, we replace stride with dilation in the last convolutional block (DC5)\")\n",
    "    parser.add_argument('--position_embedding', default='sine', type=str, choices=('sine', 'learned'),\n",
    "                        help=\"Type of positional embedding to use on top of the image features\")\n",
    "    parser.add_argument('--position_embedding_scale', default=2 * np.pi, type=float,\n",
    "                        help=\"position / size * scale\")\n",
    "    parser.add_argument('--num_feature_levels', default=4, type=int, help='number of feature levels')\n",
    "\n",
    "    # * Transformer\n",
    "    parser.add_argument('--enc_layers', default=6, type=int,\n",
    "                        help=\"Number of encoding layers in the transformer\")\n",
    "    parser.add_argument('--dec_layers', default=6, type=int,\n",
    "                        help=\"Number of decoding layers in the transformer\")\n",
    "    parser.add_argument('--dim_feedforward', default=1024, type=int,\n",
    "                        help=\"Intermediate size of the feedforward layers in the transformer blocks\")\n",
    "    parser.add_argument('--hidden_dim', default=256, type=int,\n",
    "                        help=\"Size of the embeddings (dimension of the transformer)\")\n",
    "    parser.add_argument('--dropout', default=0.1, type=float,\n",
    "                        help=\"Dropout applied in the transformer\")\n",
    "    parser.add_argument('--nheads', default=8, type=int,\n",
    "                        help=\"Number of attention heads inside the transformer's attentions\")\n",
    "    parser.add_argument('--num_queries', default=100, type=int,\n",
    "                        help=\"Number of query slots\")\n",
    "    parser.add_argument('--dec_n_points', default=4, type=int)\n",
    "    parser.add_argument('--enc_n_points', default=4, type=int)\n",
    "\n",
    "    # Loss\n",
    "    parser.add_argument('--no_aux_loss', dest='aux_loss', action='store_false',\n",
    "                        help=\"Disables auxiliary decoding losses (loss at each layer)\")\n",
    "\n",
    "    # * Matcher\n",
    "    parser.add_argument('--set_cost_class', default=2, type=float,\n",
    "                        help=\"Class coefficient in the matching cost\")\n",
    "    parser.add_argument('--set_cost_bbox', default=5, type=float,\n",
    "                        help=\"L1 box coefficient in the matching cost\")\n",
    "    parser.add_argument('--set_cost_giou', default=2, type=float,\n",
    "                        help=\"giou box coefficient in the matching cost\")\n",
    "\n",
    "    # Loss coefficients\n",
    "    parser.add_argument('--cls_loss_coef', default=2, type=float)\n",
    "    parser.add_argument('--bbox_loss_coef', default=5, type=float)\n",
    "    parser.add_argument('--giou_loss_coef', default=2, type=float)\n",
    "    parser.add_argument('--focal_alpha', default=0.25, type=float)\n",
    "    \n",
    "    # dataset parameters\n",
    "    parser.add_argument('--coco_panoptic_path', type=str)\n",
    "    parser.add_argument('--remove_difficult', action='store_true')\n",
    "    parser.add_argument('--output_dir', default='',\n",
    "                        help='path where to save, empty for no saving')\n",
    "    parser.add_argument('--device', default='cuda',\n",
    "                        help='device to use for training / testing')\n",
    "    parser.add_argument('--seed', default=42, type=int)\n",
    "    parser.add_argument('--resume', default='', help='resume from checkpoint')\n",
    "    parser.add_argument('--start_epoch', default=0, type=int, metavar='N',\n",
    "                        help='start epoch')\n",
    "    parser.add_argument('--eval', action='store_true')\n",
    "    parser.add_argument('--viz', action='store_true')\n",
    "    parser.add_argument('--eval_every', default=5, type=int)\n",
    "    parser.add_argument('--num_workers', default=3, type=int)\n",
    "    parser.add_argument('--cache_mode', default=False, action='store_true', help='whether to cache images on memory')\n",
    "    \n",
    "    ################ OW-DETR ################\n",
    "    parser.add_argument('--PREV_INTRODUCED_CLS', default=0, type=int)\n",
    "    parser.add_argument('--CUR_INTRODUCED_CLS', default=20, type=int)\n",
    "    parser.add_argument('--unmatched_boxes', default=False, action='store_true')\n",
    "    parser.add_argument('--top_unk', default=5, type=int)\n",
    "    parser.add_argument('--featdim', default=1024, type=int)\n",
    "    parser.add_argument('--invalid_cls_logits', default=False, action='store_true', help='owod setting')\n",
    "    parser.add_argument('--NC_branch', default=False, action='store_true')\n",
    "    parser.add_argument('--bbox_thresh', default=0.3, type=float)\n",
    "    parser.add_argument('--pretrain', default='', help='initialized from the pre-training model')\n",
    "    parser.add_argument('--nc_loss_coef', default=2, type=float)\n",
    "    parser.add_argument('--train_set', default='owod_t1_5classes_train', help='training txt files')\n",
    "    parser.add_argument('--test_set', default='owdetr_test', help='testing txt files')\n",
    "    parser.add_argument('--num_classes', default=81, type=int)\n",
    "    parser.add_argument('--nc_epoch', default=0, type=int)\n",
    "    parser.add_argument('--dataset', default='OWDETR', help='defines which dataset is used. Built for: {TOWOD, OWDETR, VOC2007}')\n",
    "    parser.add_argument('--data_root', default='./data/OWOD', type=str)\n",
    "    parser.add_argument('--unk_conf_w', default=1.0, type=float)\n",
    "\n",
    "    ################ PROB OWOD ################\n",
    "    # model config\n",
    "    parser.add_argument('--model_type', default='prob', type=str)\n",
    "    \n",
    "    # logging\n",
    "    parser.add_argument('--wandb_name', default='', type=str)\n",
    "    parser.add_argument('--wandb_project', default='', type=str)\n",
    "    \n",
    "    # model hyperparameters\n",
    "    parser.add_argument('--obj_loss_coef', default=1, type=float)\n",
    "    parser.add_argument('--obj_temp', default=1, type=float)\n",
    "    parser.add_argument('--freeze_prob_model', default=False, action='store_true', help='freeze model probabistic estimation')\n",
    "\n",
    "    \n",
    "    # Exemplar replay selection\n",
    "    parser.add_argument('--num_inst_per_class', default=50, type=int, help=\"number of instances per class\")\n",
    "    parser.add_argument('--exemplar_replay_selection', default=False, action='store_true', help='use learned exemplar selection')\n",
    "    parser.add_argument('--exemplar_replay_max_length', default=1e10, type=int, help=\"max number of images that can be saves\")\n",
    "    parser.add_argument('--exemplar_replay_dir', default='', type=str, help=\"directory of exemplar replay txt files\")\n",
    "    parser.add_argument('--exemplar_replay_prev_file', default='', type=str, help=\"path to previous ft file\")\n",
    "    parser.add_argument('--exemplar_replay_cur_file', default='', type=str, help=\"path to current ft file\")\n",
    "    parser.add_argument('--exemplar_replay_random', default=False, action='store_true', help='make selection random')\n",
    "\n",
    "    return parser\n",
    "\n",
    "parser = argparse.ArgumentParser('Deformable DETR training and evaluation script', parents=[get_args_parser()])\n",
    "args = parser.parse_args(args = [])\n",
    "\n",
    "print(str(args).replace(', ', '\\n'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 환경 변수에 따른 분산 학습 설정\n",
    "- 병렬 학습을 사용하지 않는 다면 실행하지 않아도 됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not using distributed mode\n",
      "Namespace(lr=0.0002, lr_backbone_names=['backbone.0'], lr_backbone=2e-05, lr_linear_proj_names=['reference_points', 'sampling_offsets'], lr_linear_proj_mult=0.1, batch_size=2, weight_decay=0.0001, epochs=51, lr_drop=35, lr_drop_epochs=None, clip_max_norm=0.1, sgd=False, with_box_refine=False, two_stage=False, masks=False, backbone='dino_resnet50', frozen_weights=None, dilation=False, position_embedding='sine', position_embedding_scale=6.283185307179586, num_feature_levels=4, enc_layers=6, dec_layers=6, dim_feedforward=1024, hidden_dim=256, dropout=0.1, nheads=8, num_queries=100, dec_n_points=4, enc_n_points=4, aux_loss=True, set_cost_class=2, set_cost_bbox=5, set_cost_giou=2, cls_loss_coef=2, bbox_loss_coef=5, giou_loss_coef=2, focal_alpha=0.25, coco_panoptic_path=None, remove_difficult=False, output_dir='', device='cuda', seed=42, resume='', start_epoch=0, eval=False, viz=False, eval_every=5, num_workers=3, cache_mode=False, PREV_INTRODUCED_CLS=0, CUR_INTRODUCED_CLS=20, unmatched_boxes=False, top_unk=5, featdim=1024, invalid_cls_logits=False, NC_branch=False, bbox_thresh=0.3, pretrain='', nc_loss_coef=2, train_set='owod_t1_5classes_train', test_set='owdetr_test', num_classes=81, nc_epoch=0, dataset='OWDETR', data_root='./data/OWOD', unk_conf_w=1.0, model_type='prob', wandb_name='', wandb_project='', obj_loss_coef=1, obj_temp=1, freeze_prob_model=False, num_inst_per_class=50, exemplar_replay_selection=False, exemplar_replay_max_length=10000000000.0, exemplar_replay_dir='', exemplar_replay_prev_file='', exemplar_replay_cur_file='', exemplar_replay_random=False, distributed=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (2.2.3) or chardet (3.0.4) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import subprocess\n",
    "import util.misc as utils\n",
    "\n",
    "utils.init_distributed_mode(args)\n",
    "\n",
    "# 가중치가 얼어있으면 segmentation 이용하라고 알려주기\n",
    "if args.frozen_weights is not None:\n",
    "        assert args.masks, \"Frozen training is meant for segmentation only\"\n",
    "print(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 재현 가능한 결과를 보장하기 위해서 seed 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "device = torch.device(args.device)\n",
    "\n",
    "# fix the seed for reproducibility\n",
    "seed = args.seed + utils.get_rank()\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pytorch 모델을 빌드하는 부분"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid class range: [20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79]\n",
      "DINO resnet50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/.local/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/workspace/.local/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running with exemplar_replay_selection\n",
      "DeformableDETR(\n",
      "  (transformer): DeformableTransformer(\n",
      "    (encoder): DeformableTransformerEncoder(\n",
      "      (layers): ModuleList(\n",
      "        (0): DeformableTransformerEncoderLayer(\n",
      "          (self_attn): MSDeformAttn(\n",
      "            (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (attention_weights): Linear(in_features=256, out_features=128, bias=True)\n",
      "            (value_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (output_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "          )\n",
      "          (dropout1): Dropout(p=0.1, inplace=False)\n",
      "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (linear1): Linear(in_features=256, out_features=1024, bias=True)\n",
      "          (dropout2): Dropout(p=0.1, inplace=False)\n",
      "          (linear2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "          (dropout3): Dropout(p=0.1, inplace=False)\n",
      "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (1): DeformableTransformerEncoderLayer(\n",
      "          (self_attn): MSDeformAttn(\n",
      "            (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (attention_weights): Linear(in_features=256, out_features=128, bias=True)\n",
      "            (value_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (output_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "          )\n",
      "          (dropout1): Dropout(p=0.1, inplace=False)\n",
      "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (linear1): Linear(in_features=256, out_features=1024, bias=True)\n",
      "          (dropout2): Dropout(p=0.1, inplace=False)\n",
      "          (linear2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "          (dropout3): Dropout(p=0.1, inplace=False)\n",
      "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (2): DeformableTransformerEncoderLayer(\n",
      "          (self_attn): MSDeformAttn(\n",
      "            (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (attention_weights): Linear(in_features=256, out_features=128, bias=True)\n",
      "            (value_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (output_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "          )\n",
      "          (dropout1): Dropout(p=0.1, inplace=False)\n",
      "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (linear1): Linear(in_features=256, out_features=1024, bias=True)\n",
      "          (dropout2): Dropout(p=0.1, inplace=False)\n",
      "          (linear2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "          (dropout3): Dropout(p=0.1, inplace=False)\n",
      "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (3): DeformableTransformerEncoderLayer(\n",
      "          (self_attn): MSDeformAttn(\n",
      "            (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (attention_weights): Linear(in_features=256, out_features=128, bias=True)\n",
      "            (value_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (output_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "          )\n",
      "          (dropout1): Dropout(p=0.1, inplace=False)\n",
      "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (linear1): Linear(in_features=256, out_features=1024, bias=True)\n",
      "          (dropout2): Dropout(p=0.1, inplace=False)\n",
      "          (linear2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "          (dropout3): Dropout(p=0.1, inplace=False)\n",
      "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (4): DeformableTransformerEncoderLayer(\n",
      "          (self_attn): MSDeformAttn(\n",
      "            (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (attention_weights): Linear(in_features=256, out_features=128, bias=True)\n",
      "            (value_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (output_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "          )\n",
      "          (dropout1): Dropout(p=0.1, inplace=False)\n",
      "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (linear1): Linear(in_features=256, out_features=1024, bias=True)\n",
      "          (dropout2): Dropout(p=0.1, inplace=False)\n",
      "          (linear2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "          (dropout3): Dropout(p=0.1, inplace=False)\n",
      "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (5): DeformableTransformerEncoderLayer(\n",
      "          (self_attn): MSDeformAttn(\n",
      "            (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (attention_weights): Linear(in_features=256, out_features=128, bias=True)\n",
      "            (value_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (output_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "          )\n",
      "          (dropout1): Dropout(p=0.1, inplace=False)\n",
      "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (linear1): Linear(in_features=256, out_features=1024, bias=True)\n",
      "          (dropout2): Dropout(p=0.1, inplace=False)\n",
      "          (linear2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "          (dropout3): Dropout(p=0.1, inplace=False)\n",
      "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (decoder): DeformableTransformerDecoder(\n",
      "      (layers): ModuleList(\n",
      "        (0): DeformableTransformerDecoderLayer(\n",
      "          (cross_attn): MSDeformAttn(\n",
      "            (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (attention_weights): Linear(in_features=256, out_features=128, bias=True)\n",
      "            (value_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (output_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "          )\n",
      "          (dropout1): Dropout(p=0.1, inplace=False)\n",
      "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
      "          )\n",
      "          (dropout2): Dropout(p=0.1, inplace=False)\n",
      "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (linear1): Linear(in_features=256, out_features=1024, bias=True)\n",
      "          (dropout3): Dropout(p=0.1, inplace=False)\n",
      "          (linear2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "          (dropout4): Dropout(p=0.1, inplace=False)\n",
      "          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (1): DeformableTransformerDecoderLayer(\n",
      "          (cross_attn): MSDeformAttn(\n",
      "            (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (attention_weights): Linear(in_features=256, out_features=128, bias=True)\n",
      "            (value_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (output_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "          )\n",
      "          (dropout1): Dropout(p=0.1, inplace=False)\n",
      "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
      "          )\n",
      "          (dropout2): Dropout(p=0.1, inplace=False)\n",
      "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (linear1): Linear(in_features=256, out_features=1024, bias=True)\n",
      "          (dropout3): Dropout(p=0.1, inplace=False)\n",
      "          (linear2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "          (dropout4): Dropout(p=0.1, inplace=False)\n",
      "          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (2): DeformableTransformerDecoderLayer(\n",
      "          (cross_attn): MSDeformAttn(\n",
      "            (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (attention_weights): Linear(in_features=256, out_features=128, bias=True)\n",
      "            (value_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (output_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "          )\n",
      "          (dropout1): Dropout(p=0.1, inplace=False)\n",
      "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
      "          )\n",
      "          (dropout2): Dropout(p=0.1, inplace=False)\n",
      "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (linear1): Linear(in_features=256, out_features=1024, bias=True)\n",
      "          (dropout3): Dropout(p=0.1, inplace=False)\n",
      "          (linear2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "          (dropout4): Dropout(p=0.1, inplace=False)\n",
      "          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (3): DeformableTransformerDecoderLayer(\n",
      "          (cross_attn): MSDeformAttn(\n",
      "            (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (attention_weights): Linear(in_features=256, out_features=128, bias=True)\n",
      "            (value_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (output_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "          )\n",
      "          (dropout1): Dropout(p=0.1, inplace=False)\n",
      "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
      "          )\n",
      "          (dropout2): Dropout(p=0.1, inplace=False)\n",
      "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (linear1): Linear(in_features=256, out_features=1024, bias=True)\n",
      "          (dropout3): Dropout(p=0.1, inplace=False)\n",
      "          (linear2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "          (dropout4): Dropout(p=0.1, inplace=False)\n",
      "          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (4): DeformableTransformerDecoderLayer(\n",
      "          (cross_attn): MSDeformAttn(\n",
      "            (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (attention_weights): Linear(in_features=256, out_features=128, bias=True)\n",
      "            (value_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (output_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "          )\n",
      "          (dropout1): Dropout(p=0.1, inplace=False)\n",
      "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
      "          )\n",
      "          (dropout2): Dropout(p=0.1, inplace=False)\n",
      "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (linear1): Linear(in_features=256, out_features=1024, bias=True)\n",
      "          (dropout3): Dropout(p=0.1, inplace=False)\n",
      "          (linear2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "          (dropout4): Dropout(p=0.1, inplace=False)\n",
      "          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (5): DeformableTransformerDecoderLayer(\n",
      "          (cross_attn): MSDeformAttn(\n",
      "            (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (attention_weights): Linear(in_features=256, out_features=128, bias=True)\n",
      "            (value_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (output_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "          )\n",
      "          (dropout1): Dropout(p=0.1, inplace=False)\n",
      "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
      "          )\n",
      "          (dropout2): Dropout(p=0.1, inplace=False)\n",
      "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (linear1): Linear(in_features=256, out_features=1024, bias=True)\n",
      "          (dropout3): Dropout(p=0.1, inplace=False)\n",
      "          (linear2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "          (dropout4): Dropout(p=0.1, inplace=False)\n",
      "          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (reference_points): Linear(in_features=256, out_features=2, bias=True)\n",
      "  )\n",
      "  (class_embed): ModuleList(\n",
      "    (0): Linear(in_features=256, out_features=81, bias=True)\n",
      "    (1): Linear(in_features=256, out_features=81, bias=True)\n",
      "    (2): Linear(in_features=256, out_features=81, bias=True)\n",
      "    (3): Linear(in_features=256, out_features=81, bias=True)\n",
      "    (4): Linear(in_features=256, out_features=81, bias=True)\n",
      "    (5): Linear(in_features=256, out_features=81, bias=True)\n",
      "  )\n",
      "  (bbox_embed): ModuleList(\n",
      "    (0): MLP(\n",
      "      (layers): ModuleList(\n",
      "        (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "        (1): Linear(in_features=256, out_features=256, bias=True)\n",
      "        (2): Linear(in_features=256, out_features=4, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (1): MLP(\n",
      "      (layers): ModuleList(\n",
      "        (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "        (1): Linear(in_features=256, out_features=256, bias=True)\n",
      "        (2): Linear(in_features=256, out_features=4, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (2): MLP(\n",
      "      (layers): ModuleList(\n",
      "        (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "        (1): Linear(in_features=256, out_features=256, bias=True)\n",
      "        (2): Linear(in_features=256, out_features=4, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (3): MLP(\n",
      "      (layers): ModuleList(\n",
      "        (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "        (1): Linear(in_features=256, out_features=256, bias=True)\n",
      "        (2): Linear(in_features=256, out_features=4, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (4): MLP(\n",
      "      (layers): ModuleList(\n",
      "        (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "        (1): Linear(in_features=256, out_features=256, bias=True)\n",
      "        (2): Linear(in_features=256, out_features=4, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (5): MLP(\n",
      "      (layers): ModuleList(\n",
      "        (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "        (1): Linear(in_features=256, out_features=256, bias=True)\n",
      "        (2): Linear(in_features=256, out_features=4, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (prob_obj_head): ModuleList(\n",
      "    (0): ProbObjectnessHead(\n",
      "      (flatten): Flatten(start_dim=0, end_dim=1)\n",
      "      (objectness_bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "    )\n",
      "    (1): ProbObjectnessHead(\n",
      "      (flatten): Flatten(start_dim=0, end_dim=1)\n",
      "      (objectness_bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "    )\n",
      "    (2): ProbObjectnessHead(\n",
      "      (flatten): Flatten(start_dim=0, end_dim=1)\n",
      "      (objectness_bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "    )\n",
      "    (3): ProbObjectnessHead(\n",
      "      (flatten): Flatten(start_dim=0, end_dim=1)\n",
      "      (objectness_bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "    )\n",
      "    (4): ProbObjectnessHead(\n",
      "      (flatten): Flatten(start_dim=0, end_dim=1)\n",
      "      (objectness_bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "    )\n",
      "    (5): ProbObjectnessHead(\n",
      "      (flatten): Flatten(start_dim=0, end_dim=1)\n",
      "      (objectness_bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (query_embed): Embedding(100, 512)\n",
      "  (input_proj): ModuleList(\n",
      "    (0): Sequential(\n",
      "      (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
      "    )\n",
      "    (1): Sequential(\n",
      "      (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
      "    )\n",
      "    (2): Sequential(\n",
      "      (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
      "    )\n",
      "    (3): Sequential(\n",
      "      (0): Conv2d(2048, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "      (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
      "    )\n",
      "  )\n",
      "  (backbone): Joiner(\n",
      "    (0): Backbone(\n",
      "      (body): IntermediateLayerGetter(\n",
      "        (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "        (bn1): FrozenBatchNorm2d()\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "        (layer1): Sequential(\n",
      "          (0): Bottleneck(\n",
      "            (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn1): FrozenBatchNorm2d()\n",
      "            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): FrozenBatchNorm2d()\n",
      "            (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn3): FrozenBatchNorm2d()\n",
      "            (relu): ReLU(inplace=True)\n",
      "            (downsample): Sequential(\n",
      "              (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (1): FrozenBatchNorm2d()\n",
      "            )\n",
      "          )\n",
      "          (1): Bottleneck(\n",
      "            (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn1): FrozenBatchNorm2d()\n",
      "            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): FrozenBatchNorm2d()\n",
      "            (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn3): FrozenBatchNorm2d()\n",
      "            (relu): ReLU(inplace=True)\n",
      "          )\n",
      "          (2): Bottleneck(\n",
      "            (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn1): FrozenBatchNorm2d()\n",
      "            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): FrozenBatchNorm2d()\n",
      "            (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn3): FrozenBatchNorm2d()\n",
      "            (relu): ReLU(inplace=True)\n",
      "          )\n",
      "        )\n",
      "        (layer2): Sequential(\n",
      "          (0): Bottleneck(\n",
      "            (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn1): FrozenBatchNorm2d()\n",
      "            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): FrozenBatchNorm2d()\n",
      "            (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn3): FrozenBatchNorm2d()\n",
      "            (relu): ReLU(inplace=True)\n",
      "            (downsample): Sequential(\n",
      "              (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "              (1): FrozenBatchNorm2d()\n",
      "            )\n",
      "          )\n",
      "          (1): Bottleneck(\n",
      "            (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn1): FrozenBatchNorm2d()\n",
      "            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): FrozenBatchNorm2d()\n",
      "            (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn3): FrozenBatchNorm2d()\n",
      "            (relu): ReLU(inplace=True)\n",
      "          )\n",
      "          (2): Bottleneck(\n",
      "            (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn1): FrozenBatchNorm2d()\n",
      "            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): FrozenBatchNorm2d()\n",
      "            (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn3): FrozenBatchNorm2d()\n",
      "            (relu): ReLU(inplace=True)\n",
      "          )\n",
      "          (3): Bottleneck(\n",
      "            (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn1): FrozenBatchNorm2d()\n",
      "            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): FrozenBatchNorm2d()\n",
      "            (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn3): FrozenBatchNorm2d()\n",
      "            (relu): ReLU(inplace=True)\n",
      "          )\n",
      "        )\n",
      "        (layer3): Sequential(\n",
      "          (0): Bottleneck(\n",
      "            (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn1): FrozenBatchNorm2d()\n",
      "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): FrozenBatchNorm2d()\n",
      "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn3): FrozenBatchNorm2d()\n",
      "            (relu): ReLU(inplace=True)\n",
      "            (downsample): Sequential(\n",
      "              (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "              (1): FrozenBatchNorm2d()\n",
      "            )\n",
      "          )\n",
      "          (1): Bottleneck(\n",
      "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn1): FrozenBatchNorm2d()\n",
      "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): FrozenBatchNorm2d()\n",
      "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn3): FrozenBatchNorm2d()\n",
      "            (relu): ReLU(inplace=True)\n",
      "          )\n",
      "          (2): Bottleneck(\n",
      "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn1): FrozenBatchNorm2d()\n",
      "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): FrozenBatchNorm2d()\n",
      "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn3): FrozenBatchNorm2d()\n",
      "            (relu): ReLU(inplace=True)\n",
      "          )\n",
      "          (3): Bottleneck(\n",
      "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn1): FrozenBatchNorm2d()\n",
      "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): FrozenBatchNorm2d()\n",
      "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn3): FrozenBatchNorm2d()\n",
      "            (relu): ReLU(inplace=True)\n",
      "          )\n",
      "          (4): Bottleneck(\n",
      "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn1): FrozenBatchNorm2d()\n",
      "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): FrozenBatchNorm2d()\n",
      "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn3): FrozenBatchNorm2d()\n",
      "            (relu): ReLU(inplace=True)\n",
      "          )\n",
      "          (5): Bottleneck(\n",
      "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn1): FrozenBatchNorm2d()\n",
      "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): FrozenBatchNorm2d()\n",
      "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn3): FrozenBatchNorm2d()\n",
      "            (relu): ReLU(inplace=True)\n",
      "          )\n",
      "        )\n",
      "        (layer4): Sequential(\n",
      "          (0): Bottleneck(\n",
      "            (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn1): FrozenBatchNorm2d()\n",
      "            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): FrozenBatchNorm2d()\n",
      "            (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn3): FrozenBatchNorm2d()\n",
      "            (relu): ReLU(inplace=True)\n",
      "            (downsample): Sequential(\n",
      "              (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "              (1): FrozenBatchNorm2d()\n",
      "            )\n",
      "          )\n",
      "          (1): Bottleneck(\n",
      "            (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn1): FrozenBatchNorm2d()\n",
      "            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): FrozenBatchNorm2d()\n",
      "            (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn3): FrozenBatchNorm2d()\n",
      "            (relu): ReLU(inplace=True)\n",
      "          )\n",
      "          (2): Bottleneck(\n",
      "            (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn1): FrozenBatchNorm2d()\n",
      "            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): FrozenBatchNorm2d()\n",
      "            (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn3): FrozenBatchNorm2d()\n",
      "            (relu): ReLU(inplace=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1): PositionEmbeddingSine()\n",
      "  )\n",
      ")\n",
      "number of params: 39742295\n"
     ]
    }
   ],
   "source": [
    "from models import build_model\n",
    "\n",
    "model, criterion, postprocessors, exemplar_selection = build_model(args, mode = args.model_type)\n",
    "model.to(device)\n",
    "\n",
    "model_without_ddp = model\n",
    "print(model_without_ddp)\n",
    "n_parameters = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print('number of params:', n_parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Open World Detection 데이터셋을 처리하고 불러오는 과정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'OWDETR': ('aeroplane', 'bicycle', 'bird', 'boat', 'bus', 'car', 'cat', 'cow', 'dog', 'horse', 'motorbike', 'sheep', 'train', 'elephant', 'bear', 'zebra', 'giraffe', 'truck', 'person', 'traffic light', 'fire hydrant', 'stop sign', 'parking meter', 'bench', 'chair', 'diningtable', 'pottedplant', 'backpack', 'umbrella', 'handbag', 'tie', 'suitcase', 'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'bed', 'toilet', 'sofa', 'frisbee', 'skis', 'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket', 'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone', 'book', 'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'tvmonitor', 'bottle', 'unknown'), 'TOWOD': ('aeroplane', 'bicycle', 'bird', 'boat', 'bottle', 'bus', 'car', 'cat', 'chair', 'cow', 'diningtable', 'dog', 'horse', 'motorbike', 'person', 'pottedplant', 'sheep', 'sofa', 'train', 'tvmonitor', 'truck', 'traffic light', 'fire hydrant', 'stop sign', 'parking meter', 'bench', 'elephant', 'bear', 'zebra', 'giraffe', 'backpack', 'umbrella', 'handbag', 'tie', 'suitcase', 'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'frisbee', 'skis', 'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket', 'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'bed', 'toilet', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone', 'book', 'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'unknown'), 'VOC2007': ('aeroplane', 'bicycle', 'bird', 'boat', 'bottle', 'bus', 'car', 'cat', 'chair', 'cow', 'diningtable', 'dog', 'horse', 'motorbike', 'person', 'pottedplant', 'sheep', 'sofa', 'train', 'tvmonitor', 'truck', 'traffic light', 'fire hydrant', 'stop sign', 'parking meter', 'bench', 'elephant', 'bear', 'zebra', 'giraffe', 'backpack', 'umbrella', 'handbag', 'tie', 'suitcase', 'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'frisbee', 'skis', 'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket', 'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'bed', 'toilet', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone', 'book', 'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'unknown')}\n",
      "OWDETR\n",
      "owod_t1_5classes_train\n",
      "owdetr_test\n",
      "Dataset swOWDetection\n",
      "    Number of datapoints: 23908\n",
      "    Root location: [['train'], Compose(\n",
      "    <datasets.transforms.RandomHorizontalFlip object at 0x7d960005c3a0>\n",
      "    <datasets.transforms.RandomSelect object at 0x7d960005c5e0>\n",
      "    Compose(\n",
      "    <datasets.transforms.ToTensor object at 0x7d960005c220>\n",
      "    <datasets.transforms.Normalize object at 0x7d960005c2b0>\n",
      ")\n",
      ")]\n",
      "    [['train'], Compose(\n",
      "    <datasets.transforms.RandomHorizontalFlip object at 0x7d960005c3a0>\n",
      "    <datasets.transforms.RandomSelect object at 0x7d960005c5e0>\n",
      "    Compose(\n",
      "    <datasets.transforms.ToTensor object at 0x7d960005c220>\n",
      "    <datasets.transforms.Normalize object at 0x7d960005c2b0>\n",
      ")\n",
      ")]\n",
      "Dataset OWDetection\n",
      "    Number of datapoints: 4952\n",
      "    Root location: ./data/OWOD\n",
      "    [['test'], Compose(\n",
      "    <datasets.transforms.RandomResize object at 0x7d960005c790>\n",
      "    Compose(\n",
      "    <datasets.transforms.ToTensor object at 0x7d960005c760>\n",
      "    <datasets.transforms.Normalize object at 0x7d960005c730>\n",
      ")\n",
      ")]\n"
     ]
    }
   ],
   "source": [
    "from datasets.coco import make_coco_transforms\n",
    "from datasets.torchvision_datasets.open_world import OWDetection, swOWDetection\n",
    "\n",
    "def get_datasets(args):\n",
    "    print(args.dataset)\n",
    "\n",
    "    train_set = args.train_set\n",
    "    test_set = args.test_set\n",
    "    dataset_train = swOWDetection(args, txt_root='./', data_root=args.data_root, image_set=args.train_set, transforms=make_coco_transforms(args.train_set), dataset = args.dataset)\n",
    "    dataset_val = OWDetection(args, args.data_root, image_set=args.test_set, dataset = args.dataset, transforms=make_coco_transforms(args.test_set))\n",
    "\n",
    "    print(args.train_set)\n",
    "    print(args.test_set)\n",
    "    print(dataset_train)\n",
    "    print(dataset_val)\n",
    "\n",
    "    return dataset_train, dataset_val\n",
    "\n",
    "dataset_train, dataset_val = get_datasets(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 분산 환경을 고려한 데이터 샘플링과 데이터 로더 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets.samplers as samplers\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "if hasattr(args, 'distributed') and args.distributed:\n",
    "    if args.cache_mode:\n",
    "        sampler_train = samplers.NodeDistributedSampler(dataset_train)\n",
    "        sampler_val = samplers.NodeDistributedSampler(dataset_val, shuffle=False)\n",
    "    else:\n",
    "        sampler_train = samplers.DistributedSampler(dataset_train)\n",
    "        sampler_val = samplers.DistributedSampler(dataset_val, shuffle=False)\n",
    "else:\n",
    "    sampler_train = torch.utils.data.RandomSampler(dataset_train)\n",
    "    sampler_val = torch.utils.data.SequentialSampler(dataset_val)\n",
    "\n",
    "batch_sampler_train = torch.utils.data.BatchSampler(sampler_train, args.batch_size, drop_last=True)\n",
    "data_loader_train = DataLoader(dataset_train, batch_sampler=batch_sampler_train,\n",
    "                                collate_fn=utils.collate_fn, num_workers=args.num_workers,\n",
    "                                pin_memory=True)\n",
    "data_loader_val = DataLoader(dataset_val, args.batch_size, sampler=sampler_val,\n",
    "                                drop_last=False, collate_fn=utils.collate_fn, num_workers=args.num_workers,\n",
    "                                pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pytorch 모델의 학습 파라미터에 대해 서로 다른 학습률을 설정하는 부분"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_name_keywords(n, name_keywords):\n",
    "        out = False\n",
    "        for b in name_keywords:\n",
    "            if b in n:\n",
    "                out = True\n",
    "                break\n",
    "        return out\n",
    "\n",
    "param_dicts = [\n",
    "        {\n",
    "            \"params\":\n",
    "                [p for n, p in model_without_ddp.named_parameters()\n",
    "                 if not match_name_keywords(n, args.lr_backbone_names) and not match_name_keywords(n, args.lr_linear_proj_names) and p.requires_grad],\n",
    "            \"lr\": args.lr,\n",
    "        },\n",
    "        {\n",
    "            \"params\": [p for n, p in model_without_ddp.named_parameters() if match_name_keywords(n, args.lr_backbone_names) and p.requires_grad],\n",
    "            \"lr\": args.lr_backbone,\n",
    "        },\n",
    "        {\n",
    "            \"params\": [p for n, p in model_without_ddp.named_parameters() if match_name_keywords(n, args.lr_linear_proj_names) and p.requires_grad],\n",
    "            \"lr\": args.lr * args.lr_linear_proj_mult,\n",
    "        }\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer와 학습률 스케줄러를 설정하는 부분"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.sgd:\n",
    "        optimizer = torch.optim.SGD(param_dicts, lr=args.lr, momentum=0.9,\n",
    "                                    weight_decay=args.weight_decay)\n",
    "else:\n",
    "    optimizer = torch.optim.AdamW(param_dicts, lr=args.lr,\n",
    "                                    weight_decay=args.weight_decay)\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, args.lr_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "if hasattr(args, 'distributed') and args.distributed:\n",
    "        model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[args.gpu])\n",
    "        model_without_ddp = model.module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "from datasets import get_coco_api_from_dataset\n",
    "\n",
    "if args.dataset == \"coco_panoptic\":\n",
    "        # We also evaluate AP during panoptic training, on original coco DS\n",
    "        coco_val = datasets.coco.build(\"val\", args)\n",
    "        base_ds = get_coco_api_from_dataset(coco_val)\n",
    "elif args.dataset == \"coco\":\n",
    "    base_ds = get_coco_api_from_dataset(dataset_val)\n",
    "else:\n",
    "    base_ds = dataset_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "if args.frozen_weights is not None:\n",
    "        checkpoint = torch.load(args.frozen_weights, map_location='cpu')\n",
    "        model_without_ddp.detr.load_state_dict(checkpoint['model'])\n",
    "\n",
    "output_dir = Path(args.output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('aeroplane', 'bicycle', 'bird', 'boat', 'bus', 'car', 'cat', 'cow', 'dog', 'horse', 'motorbike', 'sheep', 'train', 'elephant', 'bear', 'zebra', 'giraffe', 'truck', 'person', 'traffic light', 'fire hydrant', 'stop sign', 'parking meter', 'bench', 'chair', 'diningtable', 'pottedplant', 'backpack', 'umbrella', 'handbag', 'tie', 'suitcase', 'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'bed', 'toilet', 'sofa', 'frisbee', 'skis', 'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket', 'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone', 'book', 'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'tvmonitor', 'bottle', 'unknown')\n"
     ]
    }
   ],
   "source": [
    "from engine import evaluate\n",
    "\n",
    "if args.pretrain:\n",
    "    print('Initialized from the pre-training model')\n",
    "    checkpoint = torch.load(args.pretrain, map_location='cpu')\n",
    "    state_dict = checkpoint['model']\n",
    "    msg = model_without_ddp.load_state_dict(state_dict, strict=False)\n",
    "    print(msg)\n",
    "    args.start_epoch = checkpoint['epoch'] + 1\n",
    "    if args.eval:\n",
    "        test_stats, coco_evaluator = evaluate(model, criterion, postprocessors, data_loader_val, base_ds, device, args.output_dir, args)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.resume:\n",
    "    if args.resume.startswith('https'):\n",
    "        checkpoint = torch.hub.load_state_dict_from_url(\n",
    "            args.resume, map_location='cpu', check_hash=True)\n",
    "    else:\n",
    "        checkpoint = torch.load(args.resume, map_location='cpu')\n",
    "    missing_keys, unexpected_keys = model_without_ddp.load_state_dict(checkpoint['model'], strict=False)\n",
    "    unexpected_keys = [k for k in unexpected_keys if not (k.endswith('total_params') or k.endswith('total_ops'))]\n",
    "    if len(missing_keys) > 0:\n",
    "        print('Missing Keys: {}'.format(missing_keys))\n",
    "    if len(unexpected_keys) > 0:\n",
    "        print('Unexpected Keys: {}'.format(unexpected_keys))\n",
    "    if not args.eval and 'optimizer' in checkpoint and 'lr_scheduler' in checkpoint and 'epoch' in checkpoint:\n",
    "        import copy\n",
    "        p_groups = copy.deepcopy(optimizer.param_groups)\n",
    "        optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "        for pg, pg_old in zip(optimizer.param_groups, p_groups):\n",
    "            pg['lr'] = pg_old['lr']\n",
    "            pg['initial_lr'] = pg_old['initial_lr']\n",
    "        print(optimizer.param_groups)\n",
    "        lr_scheduler.load_state_dict(checkpoint['lr_scheduler'])\n",
    "        # todo: this is a hack for doing experiment that resume from checkpoint and also modify lr scheduler (e.g., decrease lr in advance).\n",
    "        args.override_resumed_lr_drop = True\n",
    "        if args.override_resumed_lr_drop:\n",
    "            print('Warning: (hack) args.override_resumed_lr_drop is set to True, so args.lr_drop would override lr_drop in resumed lr_scheduler.')\n",
    "            lr_scheduler.step_size = args.lr_drop\n",
    "            lr_scheduler.base_lrs = list(map(lambda group: group['initial_lr'], optimizer.param_groups))\n",
    "        lr_scheduler.step(lr_scheduler.last_epoch)\n",
    "        args.start_epoch = checkpoint['epoch'] + 1\n",
    "    # check the resumed model\n",
    "    if (not args.eval and not args.viz and args.dataset in ['coco', 'voc']):\n",
    "        test_stats, coco_evaluator = evaluate(\n",
    "            model, criterion, postprocessors, data_loader_val, base_ds, device, args.output_dir, args\n",
    "        )\n",
    "    if args.eval:\n",
    "        test_stats, coco_evaluator = evaluate(model, criterion, postprocessors, data_loader_val, base_ds, device, args.output_dir, args)\n",
    "        if args.output_dir:\n",
    "            utils.save_on_master(coco_evaluator.coco_eval[\"bbox\"].eval, output_dir / \"eval.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.freeze_prob_model:           \n",
    "    if isinstance(model_without_ddp.prob_obj_head, torch.nn.ModuleList):\n",
    "        for obj_head in model_without_ddp.prob_obj_head:\n",
    "            obj_head.freeze_prob_model()\n",
    "    else:\n",
    "        model_without_ddp.prob_obj_head.freeze_prob_model()\n",
    "        \n",
    "    obj_bn_mean_before=model_without_ddp.prob_obj_head[0].objectness_bn.running_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training from epoch 0 to 51\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Caught RuntimeError in DataLoader worker process 1.\nOriginal Traceback (most recent call last):\n  File \"/workspace/.local/lib/python3.10/site-packages/torch/utils/data/_utils/worker.py\", line 302, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/workspace/.local/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\", line 49, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/workspace/.local/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\", line 49, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/workspace/PROB/datasets/torchvision_datasets/open_world.py\", line 536, in __getitem__\n    img, target = self.transforms[-1](img, target)\n  File \"/workspace/PROB/datasets/transforms.py\", line 274, in __call__\n    image, target = t(image, target)\n  File \"/workspace/PROB/datasets/transforms.py\", line 232, in __call__\n    return self.transforms2(img, target)\n  File \"/workspace/PROB/datasets/transforms.py\", line 274, in __call__\n    image, target = t(image, target)\n  File \"/workspace/PROB/datasets/transforms.py\", line 206, in __call__\n    return resize(img, target, size, self.max_size)\n  File \"/workspace/PROB/datasets/transforms.py\", line 124, in resize\n    scaled_boxes = boxes * torch.as_tensor([ratio_width, ratio_height, ratio_width, ratio_height])\nRuntimeError: The size of tensor a (0) must match the size of tensor b (4) at non-singleton dimension 0\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 81\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m  \u001b[38;5;28mhasattr\u001b[39m(args, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdistributed\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m args\u001b[38;5;241m.\u001b[39mdistributed:\n\u001b[1;32m     79\u001b[0m     sampler_train\u001b[38;5;241m.\u001b[39mset_epoch(epoch)\n\u001b[0;32m---> 81\u001b[0m train_stats \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     82\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_loader_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnc_epoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclip_max_norm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwandb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     84\u001b[0m lr_scheduler\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m args\u001b[38;5;241m.\u001b[39moutput_dir:\n",
      "File \u001b[0;32m~/PROB/engine.py:43\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[0;34m(model, criterion, data_loader, optimizer, device, epoch, nc_epoch, max_norm, wandb)\u001b[0m\n\u001b[1;32m     41\u001b[0m print_freq \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m\n\u001b[1;32m     42\u001b[0m prefetcher \u001b[38;5;241m=\u001b[39m data_prefetcher(data_loader, device, prefetch\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 43\u001b[0m samples, targets \u001b[38;5;241m=\u001b[39m \u001b[43mprefetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnext\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;66;03m# tqdm progress bar for displaying the progress\u001b[39;00m\n\u001b[1;32m     46\u001b[0m progress_bar \u001b[38;5;241m=\u001b[39m tqdm(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(data_loader)), desc\u001b[38;5;241m=\u001b[39mheader)\n",
      "File \u001b[0;32m~/PROB/datasets/data_prefetcher.py:62\u001b[0m, in \u001b[0;36mdata_prefetcher.next\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     60\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m t\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m     61\u001b[0m                 v\u001b[38;5;241m.\u001b[39mrecord_stream(torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mcurrent_stream())\n\u001b[0;32m---> 62\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpreload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/PROB/datasets/data_prefetcher.py:25\u001b[0m, in \u001b[0;36mdata_prefetcher.preload\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpreload\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 25\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnext_samples, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnext_targets \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m     27\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnext_samples \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py:681\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    678\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    679\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    680\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 681\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    682\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    683\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    684\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    685\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1356\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1354\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_task_info[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_rcvd_idx]) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m   1355\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_task_info\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_rcvd_idx)[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m-> 1356\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1358\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m   1359\u001b[0m idx, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_data()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1402\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1400\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_try_put_index()\n\u001b[1;32m   1401\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ExceptionWrapper):\n\u001b[0;32m-> 1402\u001b[0m     \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1403\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_utils.py:461\u001b[0m, in \u001b[0;36mExceptionWrapper.reraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    457\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    458\u001b[0m     \u001b[38;5;66;03m# If the exception takes multiple arguments, don't try to\u001b[39;00m\n\u001b[1;32m    459\u001b[0m     \u001b[38;5;66;03m# instantiate since we don't know how to\u001b[39;00m\n\u001b[1;32m    460\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 461\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exception\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Caught RuntimeError in DataLoader worker process 1.\nOriginal Traceback (most recent call last):\n  File \"/workspace/.local/lib/python3.10/site-packages/torch/utils/data/_utils/worker.py\", line 302, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/workspace/.local/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\", line 49, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/workspace/.local/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\", line 49, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/workspace/PROB/datasets/torchvision_datasets/open_world.py\", line 536, in __getitem__\n    img, target = self.transforms[-1](img, target)\n  File \"/workspace/PROB/datasets/transforms.py\", line 274, in __call__\n    image, target = t(image, target)\n  File \"/workspace/PROB/datasets/transforms.py\", line 232, in __call__\n    return self.transforms2(img, target)\n  File \"/workspace/PROB/datasets/transforms.py\", line 274, in __call__\n    image, target = t(image, target)\n  File \"/workspace/PROB/datasets/transforms.py\", line 206, in __call__\n    return resize(img, target, size, self.max_size)\n  File \"/workspace/PROB/datasets/transforms.py\", line 124, in resize\n    scaled_boxes = boxes * torch.as_tensor([ratio_width, ratio_height, ratio_width, ratio_height])\nRuntimeError: The size of tensor a (0) must match the size of tensor b (4) at non-singleton dimension 0\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import json\n",
    "import wandb\n",
    "import datetime\n",
    "from engine import train_one_epoch, get_exemplar_replay\n",
    "\n",
    "if len(args.wandb_project)>0:\n",
    "    if len(args.wandb_name)>0:\n",
    "        wandb.init(project=args.wandb_project, entity=\"marvl\", group=args.wandb_name)\n",
    "    else:\n",
    "        wandb.init(project=args.wandb_project, entity=\"marvl\")\n",
    "    wandb.config = args\n",
    "else:\n",
    "    wandb=None\n",
    "\n",
    "def create_ft_dataset(args, image_sorted_scores):\n",
    "    print(f'found a total of {len(image_sorted_scores.keys())} images')\n",
    "    tmp_dir=args.data_root +'/ImageSets/'+args.dataset+\"/\"+args.exemplar_replay_dir+\"/\"\n",
    "    #tmp_dir=args.data_root +'/ImageSets/'+args.exemplar_replay_dir+\"/\"\n",
    "\n",
    "    class_sorted_scores={}\n",
    "    imgs_per_class={}\n",
    "    for i in range(args.PREV_INTRODUCED_CLS, args.CUR_INTRODUCED_CLS+args.PREV_INTRODUCED_CLS):\n",
    "        class_sorted_scores[str(i)]=[]\n",
    "        imgs_per_class[str(i)]=[]\n",
    "\n",
    "    for k,v in image_sorted_scores.items():\n",
    "        for j in range(len(v['labels'])):\n",
    "            class_sorted_scores[str(v['labels'][j])].append(v['scores'][j])\n",
    "\n",
    "\n",
    "    class_threshold={}\n",
    "    for i in range(args.PREV_INTRODUCED_CLS, args.CUR_INTRODUCED_CLS+args.PREV_INTRODUCED_CLS):\n",
    "        tmp=np.array(class_sorted_scores[str(i)])\n",
    "        tmp.sort()\n",
    "        tmp = torch.Tensor(tmp)\n",
    "        if len(tmp)>args.num_inst_per_class and not args.exemplar_replay_random:\n",
    "            max_val = tmp[-args.num_inst_per_class//2]\n",
    "            min_val = tmp[args.num_inst_per_class//2]\n",
    "        else:\n",
    "            if args.exemplar_replay_random:\n",
    "                print('using random exemplar selection')\n",
    "            else:\n",
    "                print(f'only found {len(tmp)} imgs in class {i}')\n",
    "            max_val = tmp.min()\n",
    "            min_val = tmp.max()\n",
    "            \n",
    "        class_threshold[str(i)]=(min_val, max_val)\n",
    "\n",
    "    save_imgs = []    \n",
    "    for k,v in image_sorted_scores.items():\n",
    "        for j in range(len(v['labels'])):\n",
    "            label = str(v['labels'][j])\n",
    "            if (v['scores'][j] <= class_threshold[label][0].numpy() or v['scores'][j] >= class_threshold[label][1].numpy()) and (len(imgs_per_class[label])<=args.num_inst_per_class+2):\n",
    "                save_imgs.append(k)\n",
    "                imgs_per_class[label].append(k)\n",
    "                        \n",
    "    print(f'found {len(np.unique(save_imgs))} images in run')\n",
    "    if len(args.exemplar_replay_prev_file)>0:\n",
    "        previous_ft = open(tmp_dir+args.exemplar_replay_prev_file,'r').read().splitlines()\n",
    "        save_imgs+=previous_ft\n",
    "        \n",
    "    save_imgs=np.unique(save_imgs)\n",
    "    np.random.shuffle(save_imgs)\n",
    "    if len(save_imgs)> args.exemplar_replay_max_length:\n",
    "        save_imgs=save_imgs[:args.exemplar_replay_max_length]\n",
    "    \n",
    "    os.makedirs(tmp_dir, exist_ok=True)\n",
    "    with open(tmp_dir+args.exemplar_replay_cur_file, 'w') as f:\n",
    "        for line in save_imgs:\n",
    "            f.write(line)\n",
    "            f.write('\\n')\n",
    "    return\n",
    "\n",
    "print(f'Start training from epoch {args.start_epoch} to {args.epochs}')\n",
    "start_time = time.time()\n",
    "for epoch in range(args.start_epoch, args.epochs):\n",
    "    if  hasattr(args, 'distributed') and args.distributed:\n",
    "        sampler_train.set_epoch(epoch)\n",
    "        \n",
    "    train_stats = train_one_epoch(\n",
    "        model, criterion, data_loader_train, optimizer, device, epoch, args.nc_epoch, args.clip_max_norm, wandb)\n",
    "    \n",
    "    lr_scheduler.step()\n",
    "    if args.output_dir:\n",
    "        checkpoint_paths = [output_dir / 'checkpoint.pth']\n",
    "        # extra checkpoint before LR drop and every 5 epochs\n",
    "        if (epoch + 1) % args.lr_drop == 0 or (epoch % args.eval_every == 0 or epoch == 0 or epoch == 1 or (args.epochs-epoch)<1):\n",
    "            test_stats, coco_evaluator = evaluate(\n",
    "                model, criterion, postprocessors, data_loader_val, base_ds, device, args.output_dir, args)\n",
    "            checkpoint_paths.append(output_dir / f'checkpoint{epoch:04}.pth')\n",
    "            if wandb is not None:\n",
    "                test_stats[\"metrics\"]['epoch']=epoch\n",
    "                wandb.log({str(key): val for key, val in test_stats[\"metrics\"].items()})\n",
    "        elif epoch > args.epochs-6:\n",
    "            checkpoint_paths.append(output_dir / f'checkpoint{epoch:04}.pth')\n",
    "            \n",
    "        else:\n",
    "                test_stats = {}\n",
    "                \n",
    "        for checkpoint_path in checkpoint_paths:\n",
    "            utils.save_on_master({\n",
    "                'model': model_without_ddp.state_dict(),\n",
    "                'optimizer': optimizer.state_dict(),\n",
    "                'lr_scheduler': lr_scheduler.state_dict(),\n",
    "                'epoch': epoch,\n",
    "                'args': args,\n",
    "            }, checkpoint_path)\n",
    "        \n",
    "    log_stats = {**{f'train_{k}': v for k, v in train_stats.items()},\n",
    "                    **{f'test_{k}': v for k, v in test_stats.items()},\n",
    "                    'epoch': epoch,\n",
    "                    'n_parameters': n_parameters}\n",
    "    \n",
    "    if args.output_dir and utils.is_main_process():\n",
    "        with (output_dir / \"log.txt\").open(\"a\") as f:\n",
    "            f.write(json.dumps(log_stats) + \"\\n\")\n",
    "        if args.dataset in ['owod', 'owdetr'] and epoch % args.eval_every == 0 and epoch > 0:\n",
    "            # for evaluation logs\n",
    "            if coco_evaluator is not None:\n",
    "                (output_dir / 'eval').mkdir(exist_ok=True)\n",
    "                if \"bbox\" in coco_evaluator.coco_eval:\n",
    "                    filenames = ['latest.pth']\n",
    "                    if epoch % 50 == 0:\n",
    "                        filenames.append(f'{epoch:03}.pth')\n",
    "                    for name in filenames:\n",
    "                        torch.save(coco_evaluator.coco_eval[\"bbox\"].eval,\n",
    "                                output_dir / \"eval\" / name)\n",
    "                        \n",
    "    if args.exemplar_replay_selection:\n",
    "        image_sorted_scores = get_exemplar_replay(model,exemplar_selection, device, data_loader_train)\n",
    "        create_ft_dataset(args, image_sorted_scores)\n",
    "            \n",
    "    total_time = time.time() - start_time\n",
    "    total_time_str = str(datetime.timedelta(seconds=int(total_time)))\n",
    "    print('Training time {}'.format(total_time_str))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
